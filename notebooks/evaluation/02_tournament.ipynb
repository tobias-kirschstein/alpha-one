{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from math import sqrt\n",
    "from collections import defaultdict\n",
    "\n",
    "from alpha_one.model.model_manager import OpenSpielModelManager, OpenSpielModelConfig\n",
    "from alpha_one.model.agent import RandomAgent, MCTSAgent\n",
    "from alpha_one.model.evaluation.agent import AgentEvaluator\n",
    "from alpha_one.utils.mcts import initialize_bot, initialize_rollout_mcts_bot\n",
    "from alpha_one.metrics import EloRatingSystem, TrueSkillRatingSystem\n",
    "from alpha_one.plots import PlotManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let several (trained) agents play games against each other and report tournament metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'connect_four'\n",
    "game = pyspiel.load_game(game_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_az_run_name = 'C4-13'\n",
    "model_manager = OpenSpielModelManager(f\"{game_name}/{model_az_run_name}\")\n",
    "print(model_manager.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_az_iteration = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load trained AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_az = model_manager.load_model(model_az_iteration)\n",
    "\n",
    "az_bot = initialize_bot(game, model_az, uct_c=sqrt(2), max_simulations=100)\n",
    "agent_az = MCTSAgent(game, az_bot, 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Build random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_random = RandomAgent(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Build MCTS agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_bot = initialize_rollout_mcts_bot(game, 1, uct_c=sqrt(2), max_simulations=100)\n",
    "agent_rollout_mcts = MCTSAgent(game, rollout_bot, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Build untrained AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_az_untrained = model_manager.build_model(OpenSpielModelConfig(game, 'mlp', 1, 1, 0, 0))\n",
    "az_bot_untrained = initialize_bot(game, model_az_untrained, uct_c=sqrt(2), max_simulations=100)\n",
    "agent_az_untrained = MCTSAgent(game, az_bot_untrained, 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent_az, agent_random, agent_rollout_mcts, agent_az_untrained]\n",
    "agent_names = [f\"Alpha Zero ({model_az_run_name})\", \"Random\", \"MCTS Rollout\", \"Alpha Zero untrained\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = AgentEvaluator(game)\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "rating_systems = [elo_rating_system, true_skill_rating_system]\n",
    "\n",
    "elo_ratings_history = []\n",
    "true_skill_ratings_history = []\n",
    "ratings_histories = [elo_ratings_history, true_skill_ratings_history]\n",
    "\n",
    "rating_system_names = ['Elo Rating', 'TrueSkill Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_evaluations = 20\n",
    "for _ in range(n_evaluations):\n",
    "    agent_id_player_1, agent_id_player_2 = np.random.choice(len(agents), replace=False, size=2)\n",
    "    match_outcome, trajectory = evaluator.evaluate(agents[agent_id_player_1], agents[agent_id_player_2])\n",
    "    match_outcome.with_renamed_players({0: agent_id_player_1, 1: agent_id_player_2})\n",
    "    \n",
    "    for rating_system, ratings_history in zip(rating_systems, ratings_histories):\n",
    "        rating_system.update_ratings([match_outcome])\n",
    "        ratings_history.append(rating_system.players.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manager = PlotManager(game_name, model_az_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rating_system, ratings_history, rating_system_name in zip(rating_systems, ratings_histories, rating_system_names):\n",
    "    plt.title(f\"Tournament ({rating_system_name})\")\n",
    "    for player_id in range(len(agents)):\n",
    "        plt.plot(range(1, len(ratings_history) + 1), [rating[player_id] for rating in ratings_history], label=agent_names[player_id])\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Matchday\")\n",
    "    plt.ylabel(rating_system_name)\n",
    "    plot_manager.save_current_plot(f\"tournament_{rating_system_name}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Elo ratings:\")\n",
    "for player_id in range(len(agents)):\n",
    "    print(f\" - {agent_names[player_id]}: {elo_rating_system.players[player_id]:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_probability_matrix = np.zeros((len(agents), len(agents)))\n",
    "for agent_1 in range(len(agents)):\n",
    "    for agent_2 in range(len(agents)):\n",
    "        win_probability_matrix[agent_1, agent_2] = elo_rating_system.calculate_win_probability(elo_rating_system.get_rating(agent_1), elo_rating_system.get_rating(agent_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.gca()\n",
    "plt.title(\"Probabilities of winning against other models\", pad=20)\n",
    "ax.matshow(win_probability_matrix, cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\", \"white\", \"green\"]))\n",
    "ax.set_xticklabels(['']+agent_names)\n",
    "ax.set_yticklabels(['']+agent_names)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
