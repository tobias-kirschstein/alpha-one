{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from alpha_one.model.model_manager import OpenSpielCheckpointManager, OpenSpielModelManager, PolicyGradientModelManager\n",
    "from alpha_one.model.agent import PolicyGradientAgent, MCTSAgent, DirectInferenceAgent\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, mcts_inference, MCTSConfig\n",
    "from alpha_one.model.evaluation import EvaluationManager\n",
    "from alpha_one.plots import PlotManager\n",
    "from env import MODEL_SAVES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"connect_four\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha Zero\n",
    "model_manager = OpenSpielModelManager(game_name, 'C4')\n",
    "\n",
    "# Policy Gradient\n",
    "model_manager = PolicyGradientModelManager(game_name)\n",
    "print(model_manager.list_runs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"PG-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = model_manager.get_checkpoint_manager(run_name)\n",
    "print(checkpoint_manager.list_checkpoints())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = checkpoint_manager.load_checkpoint(checkpoint)\n",
    "model_0 = checkpoint_manager.load_checkpoint(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup game and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game(game_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCT_C = math.sqrt(2)\n",
    "max_simulations = 100\n",
    "temperature = 0\n",
    "mcts_config = MCTSConfig(UCT_C, max_simulations, temperature)\n",
    "\n",
    "agent = MCTSAgent.from_config(game, model, mcts_config)\n",
    "# agent = DirectInferenceAgent(model)\n",
    "agent_0 = MCTSAgent.from_config(game, model_0, mcts_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient\n",
    "agent = PolicyGradientAgent(model)\n",
    "agent_0 = PolicyGradientAgent(model_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interactive play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent_mcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "player_id_model = np.random.choice(2)\n",
    "player_id_human = 1 - player_id_model\n",
    "print(f\"Welcome to a game of {game_name} against the Computer (iteration {checkpoint}). Enter 'c' to cancel the game\")\n",
    "print(f\"Player Human: {player_id_human}, Player model: {player_id_model}\")\n",
    "while not state.is_terminal():\n",
    "    current_player_str = \"Human\" if state.current_player() == player_id_human else \"Computer\"\n",
    "    print(f\"Current player: {current_player_str}\")\n",
    "    print(state.observation_string())\n",
    "    if state.current_player() == player_id_model:\n",
    "        action, policy = agent.next_move(state)\n",
    "        #if model_strategy == 'direct':\n",
    "        #    _, policy = model.inference([state.observation_tensor()], [state.legal_actions_mask()])\n",
    "        #    policy = policy[0]\n",
    "        #elif model_strategy == 'mcts':\n",
    "        #    policy = mcts_inference(game, model_0, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)\n",
    "        print(f\"Computer policy: {policy}\")\n",
    "        #action = np.random.choice(len(policy), p=policy)\n",
    "        print(f\"Computer action: {action}\")\n",
    "    else:\n",
    "        print(f\"Possible actions: {np.where(state.legal_actions_mask())[0]}\")\n",
    "        print(f\"Your action: \")\n",
    "        human_input = input()\n",
    "        if human_input == 'c':\n",
    "            break\n",
    "        else:\n",
    "            action = int(human_input)\n",
    "    state.apply_action(action)\n",
    "if not human_input == 'c':\n",
    "    winner_str = \"Human\" if state.returns()[player_id_human] == 1 else \"Computer\"\n",
    "    print(f\"The winner is: {winner_str}\")\n",
    "    print(state.observation_string(0))\n",
    "else:\n",
    "    print(\"Game was cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manager = PlotManager(game_name, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Sure win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.next_move(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "correct_move_probabilities = dict()\n",
    "prevent_win_probabilities = dict()\n",
    "correct_move_probabilities_mcts = dict()\n",
    "prevent_win_probabilities_mcts = dict()\n",
    "for iteration in checkpoint_manager.list_checkpoints():\n",
    "    model_tmp = checkpoint_manager.load_checkpoint(iteration)\n",
    "    agent_mcts = MCTSAgent.from_config(game, model_tmp, mcts_config)\n",
    "    agent_direct = DirectInferenceAgent(model_tmp\n",
    "                                       )\n",
    "    _, policy = agent_direct.next_move(state)\n",
    "    _, policy_mcts = agent_mcts.next_move(state)\n",
    "    \n",
    "    correct_move_probabilities[iteration] = policy[3]\n",
    "    prevent_win_probabilities[iteration] = policy[2]\n",
    "    correct_move_probabilities_mcts[iteration] = policy_mcts[3]\n",
    "    prevent_win_probabilities_mcts[iteration] = policy_mcts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.title('Learned Policies for scenario 1 (Sure win)')\n",
    "plt.plot(list(correct_move_probabilities.keys()), list(correct_move_probabilities.values()), label='winning move')\n",
    "plt.plot(list(correct_move_probabilities_mcts.keys()), list(correct_move_probabilities_mcts.values()), label='winning move (MCTS)', linestyle=':', c='blue')\n",
    "plt.plot(list(prevent_win_probabilities.keys()), list(prevent_win_probabilities.values()), label='prevent enemy win')\n",
    "plt.plot(list(prevent_win_probabilities_mcts.keys()), list(prevent_win_probabilities_mcts.values()), label='prevent enemy win (MCTS)', linestyle=':', c='orange')\n",
    "plt.legend()\n",
    "\n",
    "plot_manager.save_current_plot(\"policies_scenario_1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Prevent Sure win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(1)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.next_move(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, model_0, state, uct_c=UCT_C, max_simulations=100, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "prevent_win_probabilities = dict()\n",
    "prevent_win_probabilities_mcts = dict()\n",
    "for iteration in checkpoint_manager.list_checkpoints():\n",
    "    model_tmp = checkpoint_manager.load_checkpoint(iteration)\n",
    "    \n",
    "    agent_mcts = MCTSAgent.from_config(game, model_tmp, mcts_config)\n",
    "    agent_direct = DirectInferenceAgent(model_tmp\n",
    "                                       )\n",
    "    _, policy = agent_direct.next_move(state)\n",
    "    _, policy_mcts = agent_mcts.next_move(state)\n",
    "    \n",
    "    prevent_win_probabilities[iteration] = policy[2]\n",
    "    prevent_win_probabilities_mcts[iteration] = policy_mcts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Learned Policies for scenario 2 (Prevent Sure win)')\n",
    "plt.plot(list(prevent_win_probabilities.keys()), list(prevent_win_probabilities.values()), label='Prevent enemy win')\n",
    "plt.plot(list(prevent_win_probabilities_mcts.keys()), list(prevent_win_probabilities_mcts.values()), label='Prevent enemy win (MCTS)', linestyle=':', c='blue')\n",
    "plt.legend()\n",
    "\n",
    "plot_manager.save_current_plot(\"policies_scenario_2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Prevent Sure win next turn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.next_move(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "correct_move_left_probabilities = dict()\n",
    "correct_move_right_probabilities = dict()\n",
    "correct_move_left_probabilities_mcts = dict()\n",
    "correct_move_right_probabilities_mcts = dict()\n",
    "for iteration in checkpoint_manager.list_checkpoints():\n",
    "    model_tmp = checkpoint_manager.load_checkpoint(iteration)\n",
    "    \n",
    "    agent_mcts = MCTSAgent.from_config(game, model_tmp, mcts_config)\n",
    "    agent_direct = DirectInferenceAgent(model_tmp\n",
    "                                       )\n",
    "    _, policy = agent_direct.next_move(state)\n",
    "    _, policy_mcts = agent_mcts.next_move(state)\n",
    "    \n",
    "    correct_move_left_probabilities[iteration] = policy[1]\n",
    "    correct_move_left_probabilities_mcts[iteration] = policy_mcts[1]\n",
    "    correct_move_right_probabilities[iteration] = policy[4]\n",
    "    correct_move_right_probabilities_mcts[iteration] = policy_mcts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Learned Policies for scenario 3 (Prevent sure win next turn)')\n",
    "plt.plot(list(correct_move_left_probabilities.keys()), list(correct_move_left_probabilities.values()), label='Correct move left')\n",
    "plt.plot(list(correct_move_left_probabilities_mcts.keys()), list(correct_move_left_probabilities_mcts.values()), label='Correct move left (MCTS)', linestyle=':', c='blue')\n",
    "plt.plot(list(correct_move_right_probabilities.keys()), list(correct_move_right_probabilities.values()), label='Correct move right')\n",
    "plt.plot(list(correct_move_right_probabilities_mcts.keys()), list(correct_move_right_probabilities_mcts.values()), label='Correct move right (MCTS)', linestyle=':', c='orange')\n",
    "plt.legend()\n",
    "\n",
    "plot_manager.save_current_plot(\"policies_scenario_3.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Direct Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Using MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, model, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, model_0, state, uct_c=UCT_C, max_simulations=100, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Play against previous generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_manager = EvaluationManager(game, 100, mcts_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = model_manager.load_checkpoint(339)\n",
    "model_1 = model_manager.load_checkpoint(0)\n",
    "evaluation_results = evaluation_manager.compare_models(model_0, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trained model won {1 - evaluation_results[0]:0.2%} of the games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
