{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from alpha_one.model.model_manager import OpenSpielModelManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, mcts_inference, MCTSConfig\n",
    "from alpha_one.model.evaluation import EvaluationManager\n",
    "from alpha_one.plots import PlotManager\n",
    "from env import MODEL_SAVES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"connect_four\"\n",
    "run_name = \"C4-13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = OpenSpielModelManager(f\"{game_name}/{run_name}\")\n",
    "print(f\"Available models: {model_manager.list_models()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy way to load model, use this for now\n",
    "game = pyspiel.load_game(\"connect_four\")\n",
    "\n",
    "config = OpenSpielModelConfig(game, \"mlp\", 64, 4, 1e-5, 5e-4)\n",
    "model = model_manager.build_model(config)\n",
    "model_0 = model_manager.build_model(config)\n",
    "model.load_checkpoint(f\"{MODEL_SAVES_DIR}/{game_name}/{run_name}/checkpoint-{iteration}\")\n",
    "model_0.load_checkpoint(f\"{MODEL_SAVES_DIR}/{game_name}/{run_name}/checkpoint-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future way to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_manager.load_model(iteration)\n",
    "model_0 = model_manager.load_model(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game(\"connect_four\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCT_C = math.sqrt(2)\n",
    "max_simulations = 100\n",
    "temperature = 1\n",
    "mcts_config = MCTSConfig(UCT_C, max_simulations, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interactive play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_strategy = \"mcts\"  # direct or mcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "player_id_model = np.random.choice(2)\n",
    "player_id_human = 1 - player_id_model\n",
    "print(f\"Welcome to a game of {game_name} against the Computer (iteration {iteration}). Enter 'c' to cancel the game\")\n",
    "print(f\"Player Human: {player_id_human}, Player model: {player_id_model}\")\n",
    "while not state.is_terminal():\n",
    "    current_player_str = \"Human\" if state.current_player() == player_id_human else \"Computer\"\n",
    "    print(f\"Current player: {current_player_str}\")\n",
    "    print(state.observation_string())\n",
    "    if state.current_player() == player_id_model:\n",
    "        if model_strategy == 'direct':\n",
    "            _, policy = model.inference([state.observation_tensor()], [state.legal_actions_mask()])\n",
    "            policy = policy[0]\n",
    "        elif model_strategy == 'mcts':\n",
    "            policy = mcts_inference(game, model_0, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)\n",
    "        print(f\"Computer policy: {policy}\")\n",
    "        action = np.random.choice(len(policy), p=policy)\n",
    "        print(f\"Computer action: {action}\")\n",
    "    else:\n",
    "        print(f\"Possible actions: {np.where(state.legal_actions_mask())[0]}\")\n",
    "        print(f\"Your action: \")\n",
    "        human_input = input()\n",
    "        if human_input == 'c':\n",
    "            break\n",
    "        else:\n",
    "            action = int(human_input)\n",
    "    state.apply_action(action)\n",
    "if not human_input == 'c':\n",
    "    winner_str = \"Human\" if state.returns()[player_id_human] == 1 else \"Computer\"\n",
    "    print(f\"The winner is: {winner_str}\")\n",
    "    print(state.observation_string(0))\n",
    "else:\n",
    "    print(\"Game was cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manager = PlotManager(game_name, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Sure win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "correct_move_probabilities = dict()\n",
    "prevent_win_probabilities = dict()\n",
    "correct_move_probabilities_mcts = dict()\n",
    "prevent_win_probabilities_mcts = dict()\n",
    "for iteration in model_manager.list_models():\n",
    "    model_tmp = model_manager.load_model(iteration)\n",
    "    policy = model_tmp.inference([state.observation_tensor()], [state.legal_actions_mask()])[1][0]\n",
    "    policy_mcts = mcts_inference(game, model_tmp, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)\n",
    "    correct_move_probabilities[iteration] = policy[3]\n",
    "    prevent_win_probabilities[iteration] = policy[2]\n",
    "    correct_move_probabilities_mcts[iteration] = policy_mcts[3]\n",
    "    prevent_win_probabilities_mcts[iteration] = policy_mcts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.title('Learned Policies for scenario 1 (Sure win)')\n",
    "plt.plot(list(correct_move_probabilities.keys()), list(correct_move_probabilities.values()), label='winning move')\n",
    "plt.plot(list(correct_move_probabilities_mcts.keys()), list(correct_move_probabilities_mcts.values()), label='winning move (MCTS)', linestyle=':', c='blue')\n",
    "plt.plot(list(prevent_win_probabilities.keys()), list(prevent_win_probabilities.values()), label='prevent enemy win')\n",
    "plt.plot(list(prevent_win_probabilities_mcts.keys()), list(prevent_win_probabilities_mcts.values()), label='prevent enemy win (MCTS)', linestyle=':', c='orange')\n",
    "plt.legend()\n",
    "\n",
    "plot_manager.save_current_plot(\"policies_scenario_1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Prevent Sure win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(1)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, model_0, state, uct_c=UCT_C, max_simulations=10000, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "prevent_win_probabilities = dict()\n",
    "prevent_win_probabilities_mcts = dict()\n",
    "for iteration in model_manager.list_models():\n",
    "    model_tmp = model_manager.load_model(iteration)\n",
    "    policy = model_tmp.inference([state.observation_tensor()], [state.legal_actions_mask()])[1][0]\n",
    "    policy_mcts = mcts_inference(game, model_tmp, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)\n",
    "    prevent_win_probabilities[iteration] = policy[2]\n",
    "    prevent_win_probabilities_mcts[iteration] = policy_mcts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Learned Policies for scenario 2 (Prevent Sure win)')\n",
    "plt.plot(list(prevent_win_probabilities.keys()), list(prevent_win_probabilities.values()), label='Prevent enemy win')\n",
    "plt.plot(list(prevent_win_probabilities_mcts.keys()), list(prevent_win_probabilities_mcts.values()), label='Prevent enemy win (MCTS)', linestyle=':', c='blue')\n",
    "plt.legend()\n",
    "\n",
    "plot_manager.save_current_plot(\"policies_scenario_2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Prevent Sure win next turn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "correct_move_left_probabilities = dict()\n",
    "correct_move_right_probabilities = dict()\n",
    "correct_move_left_probabilities_mcts = dict()\n",
    "correct_move_right_probabilities_mcts = dict()\n",
    "for iteration in model_manager.list_models():\n",
    "    model_tmp = model_manager.load_model(iteration)\n",
    "    policy = model_tmp.inference([state.observation_tensor()], [state.legal_actions_mask()])[1][0]\n",
    "    policy_mcts = mcts_inference(game, model_tmp, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)\n",
    "    correct_move_left_probabilities[iteration] = policy[1]\n",
    "    correct_move_left_probabilities_mcts[iteration] = policy_mcts[1]\n",
    "    correct_move_right_probabilities[iteration] = policy[4]\n",
    "    correct_move_right_probabilities_mcts[iteration] = policy_mcts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Learned Policies for scenario 3 (Prevent sure win next turn)')\n",
    "plt.plot(list(correct_move_left_probabilities.keys()), list(correct_move_left_probabilities.values()), label='Correct move left')\n",
    "plt.plot(list(correct_move_left_probabilities_mcts.keys()), list(correct_move_left_probabilities_mcts.values()), label='Correct move left (MCTS)', linestyle=':', c='blue')\n",
    "plt.plot(list(correct_move_right_probabilities.keys()), list(correct_move_right_probabilities.values()), label='Correct move right')\n",
    "plt.plot(list(correct_move_right_probabilities_mcts.keys()), list(correct_move_right_probabilities_mcts.values()), label='Correct move right (MCTS)', linestyle=':', c='orange')\n",
    "plt.legend()\n",
    "\n",
    "plot_manager.save_current_plot(\"policies_scenario_3.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Direct Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Using MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, model, state, uct_c=UCT_C, max_simulations=max_simulations, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, model_0, state, uct_c=UCT_C, max_simulations=100, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Play against previous generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_manager = EvaluationManager(game, 100, mcts_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = model_manager.load_model(339)\n",
    "model_1 = model_manager.load_model(0)\n",
    "evaluation_results = evaluation_manager.compare_models(model_0, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trained model won {1 - evaluation_results[0]:0.2%} of the games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
