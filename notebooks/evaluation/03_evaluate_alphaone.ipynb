{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "from alpha_one.model.model_manager import OpenSpielCheckpointManager, AlphaOneCheckpointManager, CFRCheckpointManager\n",
    "from alpha_one.model.agent import IIGMCTSAgent, DMCTSAgent, OmniscientAgent, DirectInferenceAgent, RandomAgent, HybridAlphaOneDMCTSAgent, CFRAgent\n",
    "from alpha_one.model.evaluation.agent import AgentEvaluator \n",
    "from alpha_one.metrics import AverageRewardRatingSystem, EloRatingSystem, TrueSkillRatingSystem\n",
    "from alpha_one.train import MCTSConfig\n",
    "from alpha_one.utils.state_to_value import state_to_value\n",
    "from alpha_one.utils.mcts_II import initialize_bot_alphaone, ii_mcts_agent, IIGMCTSConfig\n",
    "from alpha_one.utils.play import GameMachine\n",
    "from alpha_one.utils.determinized_mcts import initialize_bot, compute_mcts_policy\n",
    "from alpha_one.game.information_set import InformationSetGenerator\n",
    "from alpha_one.plots import PlotManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Game Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"leduc_poker\"\n",
    "game = pyspiel.load_game(game_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCT_C = 5\n",
    "max_mcts_simulations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. AlphaOne\n",
    "LP-local-43 with checkpoint 15 seems to be good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_alpha_one = \"LP-local-43\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_previous_observations = 3\n",
    "optimism = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager_alpha_one = AlphaOneCheckpointManager(game_name, run_name_alpha_one)\n",
    "\n",
    "observation_model, game_model = model_manager_alpha_one.load_checkpoint(-1)\n",
    "observation_model_untrained, game_model_untrained = model_manager_alpha_one.load_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaone_mcts_config = IIGMCTSConfig(\n",
    "    uct_c=UCT_C, \n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    temperature=0,\n",
    "    alpha_one=True, \n",
    "    state_to_value=state_to_value(game_name), \n",
    "    use_reward_polic=True, \n",
    "    n_previous_observations=n_previous_observations, \n",
    "    optimism=optimism)\n",
    "\n",
    "alpha_one_agent = IIGMCTSAgent.from_config(game, observation_model, game_model, alphaone_mcts_config)\n",
    "alpha_one_agent_untrained = IIGMCTSAgent.from_config(game, observation_model_untrained, game_model_untrained, alphaone_mcts_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. D-MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_d_mcts = \"LP-local-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager_dmcts = OpenSpielCheckpointManager(game_name, run_name_d_mcts)\n",
    "\n",
    "dmcts_model = model_manager_dmcts.load_checkpoint(-1)\n",
    "dmcts_model_untrained = model_manager_dmcts.load_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmcts_mcts_config = MCTSConfig(uct_c=math.sqrt(2), \n",
    "                               max_mcts_simulations=100,\n",
    "                               temperature=0,\n",
    "                               determinized_MCTS=True, \n",
    "                               omniscient_observer=True,\n",
    "                               use_reward_polic=True)\n",
    "\n",
    "d_mcts_agent = DMCTSAgent(dmcts_model, dmcts_mcts_config)\n",
    "d_mcts_agent_untrained = DMCTSAgent(dmcts_model_untrained, dmcts_mcts_config)\n",
    "d_mcts_random_rollout_agent = DMCTSAgent(None, dmcts_mcts_config, n_rollouts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Omniscient Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_omniscient = \"LP-local-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager_omniscient = OpenSpielCheckpointManager(game_name, run_name_omniscient)\n",
    "\n",
    "omniscient_model = model_manager_omniscient.load_checkpoint(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniscient_mcts_config = MCTSConfig(UCT_C, max_mcts_simulations, 0, use_reward_policy=True, omniscient_observer=True)\n",
    "\n",
    "omniscient_agent_untrained = OmniscientAgent(game, omniscient_mcts_config)\n",
    "omniscient_agent = OmniscientAgent(game, omniscient_mcts_config, model=omniscient_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Blind Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_blind = \"LP-12-blind-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager_blind = OpenSpielCheckpointManager(game_name, run_name_blind)\n",
    "\n",
    "blind_model = model_manager_blind.load_checkpoint(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_agent = DirectInferenceAgent(blind_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Hybrid AlphaOne + D-MCTS (Requires 2.1. and 2.2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_alpha_one_dmcts_agent = HybridAlphaOneDMCTSAgent(dmcts_model, observation_model, dmcts_mcts_config, state_to_value(game_name), n_previous_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Hybrid AlphaOne + Omniscient (Requires 2.1. and 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_alpha_one_omniscient_agent = IIGMCTSAgent.from_config(game, observation_model, omniscient_model, alphaone_mcts_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Hybrid D-MCTS + Omniscient (Requires 2.2 and 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_d_mcts_omniscient_agent = DMCTSAgent(omniscient_model, dmcts_mcts_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Super Hybrid (D-MCTS + Omniscient + AlphaOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_hybrid_agent = HybridAlphaOneDMCTSAgent(omniscient_model, observation_model, dmcts_mcts_config, state_to_value(game_name), n_previous_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. Counterfactual Regret Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_cfr = 'LP-CFR-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = CFRCheckpointManager(game_name, run_name_cfr)\n",
    "cfr_policy_table = model_manager.load_checkpoint(-1)\n",
    "cfr_agent = CFRAgent(cfr_policy_table, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Player Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available Agents:\n",
    " - `alpha_one_agent`\n",
    " - `alpha_one_agent_untrained`\n",
    " - `d_mcts_agent`\n",
    " - `d_mcts_agent_untrained`\n",
    " - `d_mcts_random_rollout_agent`\n",
    " - `omniscient_agent`\n",
    " - `omniscient_agent_untrained`\n",
    " - `blind_agent`\n",
    " - `random_agent`\n",
    " - `hybrid_alpha_one_dmcts_agent`\n",
    " - `hybrid_alpha_one_omniscient_agent`\n",
    " - `hybrid_d_mcts_omniscient_agent`\n",
    " - `cfr_agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_setup = {\n",
    "    1: d_mcts_agent,\n",
    "    0: random_agent,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Competition with 2 players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_machine = GameMachine(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_guess = 0\n",
    "incorrect_guess = 0\n",
    "game_returns = []\n",
    "for _ in tqdm(range(50)):\n",
    "    game_machine.new_game()\n",
    "\n",
    "    while not game_machine.is_finished():\n",
    "        player = game_machine.current_player()\n",
    "        agent = player_setup[player]\n",
    "        \n",
    "        if agent.is_information_set_agent():\n",
    "            action, policy = agent.next_move(game_machine.get_information_set_generator())\n",
    "        else:\n",
    "            action, policy = agent.next_move(game_machine.get_state())\n",
    "            \n",
    "        #action = np.argmax(policy)\n",
    "        if isinstance(agent, IIGMCTSAgent):\n",
    "            guessed_state = agent.get_last_guessed_state()\n",
    "            if guessed_state.__str__() == game_machine.state.__str__():\n",
    "                correct_guess += 1\n",
    "            else:\n",
    "                incorrect_guess += 1\n",
    "    \n",
    "        game_machine.play_action(action)\n",
    "            \n",
    "    game_returns.append(game_machine.get_rewards())\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_returns = np.array(game_returns)\n",
    "average_return = game_returns.mean(axis=0)\n",
    "print(f\"Average return:\")\n",
    "print(f\"---------------\")\n",
    "print(f\"  {type(player_setup[0]).__name__}: {average_return[0]}\")\n",
    "print(f\"  {type(player_setup[1]).__name__}: {average_return[1]}\")\n",
    "if correct_guess + incorrect_guess > 0:\n",
    "    print(f\" correct guess probability: {correct_guess/(correct_guess+incorrect_guess):0.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Tournament of Tournaments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    alpha_one_agent: 'AlphaOne',\n",
    "    alpha_one_agent_untrained: 'AlphaOne untrained',\n",
    "    d_mcts_agent: 'D-MCTS',\n",
    "    d_mcts_agent_untrained: 'D-MCTS untrained',\n",
    "    d_mcts_random_rollout_agent: 'D-MCTS rollout',\n",
    "    omniscient_agent: 'Omniscient',\n",
    "    omniscient_agent_untrained: 'Omniscient untrained',\n",
    "    blind_agent: 'Blind',\n",
    "    random_agent: 'Random',\n",
    "    hybrid_alpha_one_dmcts_agent: 'Hybrid AlphaOne + D-MCTS',\n",
    "    hybrid_alpha_one_omniscient_agent: 'Hybrid AlphaOne + Omniscient',\n",
    "    cfr_agent: 'CFR'\n",
    "}\n",
    "agents_by_key, agent_names_by_key = zip(*agents.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = AgentEvaluator(game)\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "average_reward_rating_system = AverageRewardRatingSystem()\n",
    "rating_systems = [elo_rating_system, true_skill_rating_system, average_reward_rating_system]\n",
    "\n",
    "elo_ratings_history = []\n",
    "true_skill_ratings_history = []\n",
    "average_reward_ratings_history = []\n",
    "ratings_histories = [elo_ratings_history, true_skill_ratings_history, average_reward_ratings_history]\n",
    "\n",
    "rating_system_names = ['Elo Rating', 'TrueSkill Rating', 'Average Reward Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_match(agent_id_player_1, agent_id_player_2):\n",
    "    match_outcome, trajectory = evaluator.evaluate(agents_by_key[agent_id_player_1], agents_by_key[agent_id_player_2])\n",
    "    match_outcome.with_renamed_players({0: agent_id_player_1, 1: agent_id_player_2})\n",
    "    return match_outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Matchday takes around 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_match_days = 20\n",
    "for _ in tqdm(range(n_match_days)):\n",
    "    match_outcomes = []\n",
    "    for agent_id_1, agent_id_2 in combinations(range(len(agents)), 2):\n",
    "        match_outcomes.append(play_match(agent_id_1, agent_id_2))\n",
    "        match_outcomes.append(play_match(agent_id_2, agent_id_1))\n",
    "            \n",
    "    for rating_system, ratings_history in zip(rating_systems, ratings_histories):\n",
    "        rating_system.update_ratings(match_outcomes)\n",
    "        ratings_history.append(rating_system.get_ratings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manager = PlotManager.new_run(game_name, 'tournament')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manager = PlotManager.new_run(game_name, 'tournament')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manager.save(elo_rating_system.get_ratings(), 'elo_ratings')\n",
    "plot_manager.save(true_skill_rating_system.get_ratings(), 'true_skill_ratings')\n",
    "plot_manager.save(average_reward_rating_system.get_ratings(), 'average_reward_ratings')\n",
    "#plot_manager.save(ratings_histories, 'ratings_histories')\n",
    "plot_manager.save(agent_names_by_key, 'agent_names_by_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [0, 0, 1, 1, 1, 2, 2, 3, 4, 1, 0, 7]\n",
    "line_style_config = {\n",
    "    1: '-',\n",
    "    2: '--',\n",
    "    3: '-.',\n",
    "    4: ':'\n",
    "}\n",
    "line_styles = [line_style_config[colors[:idx + 1].count(c)] for idx, c in enumerate(colors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rating_system, ratings_history, rating_system_name in zip(rating_systems, ratings_histories, rating_system_names):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    color_map = plt.cm.tab10(np.linspace(0,1,max(colors) + 1))\n",
    "    plt.title(f\"Tournament ({rating_system_name})\")\n",
    "    for player_id in range(len(agents)):\n",
    "        plt.plot(range(1, len(ratings_history) + 1), [rating[player_id] for rating in ratings_history], label=agent_names_by_key[player_id], color=color_map[colors[player_id]], linestyle=line_styles[player_id])\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Matchday\")\n",
    "    plt.ylabel(rating_system_name)\n",
    "    plot_manager.save_current_plot(f\"tournament_{rating_system_name}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final average rewards:\")\n",
    "average_rewards = average_reward_rating_system.get_ratings()\n",
    "average_rewards = sorted(average_rewards.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_agents, _ = zip(*average_rewards)\n",
    "for player_id in sorted_agents:\n",
    "    print(f\" - {agent_names_by_key[player_id]}: {average_reward_rating_system.get_rating(player_id):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_probability_matrix = np.zeros((len(agents), len(agents)))\n",
    "for agent_1 in range(len(agents)):\n",
    "    for agent_2 in range(len(agents)):\n",
    "        win_probability_matrix[agent_1, agent_2] = elo_rating_system.calculate_win_probability(elo_rating_system.get_rating(agent_1), elo_rating_system.get_rating(agent_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.gca()\n",
    "plt.title(\"Probabilities of winning against other models\", pad=180, fontsize=20)\n",
    "ax.matshow(win_probability_matrix, cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\", \"white\", \"green\"]))\n",
    "ax.set_xticks(list(range(12)))\n",
    "ax.set_yticks(list(range(12)))\n",
    "ax.set_xticklabels(list(agent_names_by_key), rotation=90)\n",
    "ax.set_yticklabels(list(agent_names_by_key))\n",
    "ax.axis('image')\n",
    "plt.tight_layout()\n",
    "plot_manager.save_current_plot(f\"tournament_winning_probabilities.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward_matrix = np.zeros((len(agents), len(agents)))\n",
    "for agent_1 in range(len(agents)):\n",
    "    for agent_2 in range(len(agents)):\n",
    "        average_reward_matrix[agent_1, agent_2] = average_reward_rating_system.get_rating(agent_1) - average_reward_rating_system.get_rating(agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.gca()\n",
    "plt.title(\"Average rewards against other models\", pad=180, fontsize=20)\n",
    "ax.matshow(average_reward_matrix, cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\", \"white\", \"green\"]))\n",
    "ax.set_xticks(list(range(12)))\n",
    "ax.set_yticks(list(range(12)))\n",
    "ax.set_xticklabels(list(agent_names_by_key), rotation=90)\n",
    "ax.set_yticklabels(list(agent_names_by_key))\n",
    "plt.tight_layout()\n",
    "plot_manager.save_current_plot(f\"tournament_average_rewards.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
