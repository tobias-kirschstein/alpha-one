{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem, calculate_entropy\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.game.observer import get_observation_tensor_shape\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, play_one_game, mcts_inference\n",
    "from alpha_one.utils.mcts_II import IIGMCTSConfig\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from alpha_one.utils.play import GameMachine\n",
    "from alpha_one.model.model_manager import OpenSpielCheckpointManager, OpenSpielModelManager\n",
    "from alpha_one.model.evaluation import EvaluationManager, ParallelEvaluationManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from alpha_one.train import AlphaOneTrainManager, MCTSConfig\n",
    "from alpha_one.data.replay import ReplayDataManager\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR\n",
    "\n",
    "from alpha_one.utils.state_to_value import state_to_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is done in a similar way as AlphaZero.\n",
    "\n",
    "Some Remarks:\n",
    "1. Variables with intial word \"observation\" is associated with observation NN model\n",
    "2. Variables with intial word \"game\" is associated with game (after guess state) NN model\n",
    "3. alpha_one in MCTSConfig should be set to \"True\" while training AlphaOne\n",
    "4. For the observation model, set output shape as well in model config (see below)\n",
    "5. See AlphaOneTrainManager and utilis/mcts_II for debugging\n",
    "6. Currently, evaluation doesn't support parallel evaluation\n",
    "7. Also, pass state_to_value in the MCTS config because it is used by observation NN to calculate state mask same as legal action mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'leduc_poker'\n",
    "game_prefix = 'LP-local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is state to id\n",
    "state_to_value = state_to_value(game_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_games_train = 100             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "n_games_valid = 10\n",
    "store_replays_every = 10\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_most_recent_valid_samples = 50000\n",
    "n_train_steps_obs = 1000                     # Gradient updates for observation model\n",
    "n_train_steps_game = 40                     # Gradient updates for game model\n",
    "n_valid_steps = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 10                    # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = 0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "\n",
    "# MCTS config\n",
    "UCT_C = 10                              # Exploration constant. Should be higher if absolute rewards are higher in a game\n",
    "max_mcts_simulations = 10\n",
    "optimism = 0.1                         # Whether guessing states is biased towards good outcomes\n",
    "\n",
    "policy_epsilon = None #0.25            # What noise epsilon to use\n",
    "policy_alpha = None #1                 # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10\n",
    "\n",
    "alpha_one = True\n",
    "omniscient_observer = True             # Whether the game model should have total information of the state it guessed\n",
    "use_reward_policy = True               # Whether the total rewards of nodes should be taken into account when constructing policies, or only the explore_counts\n",
    "use_teacher_forcing = True             # Whether the true game states should be used as label for the observation model, or the guessing policy of the IIG-MCTS\n",
    "n_previous_observations = 3            # How many previous observations the observation model should use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_config = IIGMCTSConfig(UCT_C, max_mcts_simulations, temperature, temperature_drop, policy_epsilon, policy_alpha, alpha_one=alpha_one, state_to_value=state_to_value, use_reward_policy=use_reward_policy, optimism=optimism, n_previous_observations=n_previous_observations)\n",
    "evaluation_mcts_config = IIGMCTSConfig(UCT_C, max_mcts_simulations, 0, None, None, None, alpha_one=alpha_one, state_to_value=state_to_value, use_reward_policy=use_reward_policy, optimism=optimism, n_previous_observations=n_previous_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "model_type_obs = 'mlp'\n",
    "nn_width_obs = 128\n",
    "nn_depth_obs = 4\n",
    "weight_decay_obs = 1e-5\n",
    "learning_rate_obs = 1e-2\n",
    "\n",
    "model_type_game = 'mlp'\n",
    "nn_width_game = 64\n",
    "nn_depth_game = 2\n",
    "weight_decay_game = 1e-5\n",
    "learning_rate_game = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    game_name=game_name,\n",
    "    UCT_C=UCT_C,\n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    n_iterations=n_iterations,\n",
    "    \n",
    "    n_games_train=n_games_train,\n",
    "    n_games_valid=n_games_valid,\n",
    "    store_replays_every=store_replays_every,\n",
    "    \n",
    "    n_most_recent_train_samples=n_most_recent_train_samples,\n",
    "    n_most_recent_valid_samples=n_most_recent_valid_samples,\n",
    "    n_train_steps_obs=n_train_steps_obs,\n",
    "    n_train_steps_game=n_train_steps_game,\n",
    "    n_valid_steps=n_valid_steps,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    n_evaluations=n_evaluations,\n",
    "    win_ratio_needed=win_ratio_needed,\n",
    "    \n",
    "    policy_epsilon=policy_epsilon,\n",
    "    policy_alpha=policy_alpha,\n",
    "    \n",
    "    temperature=temperature,\n",
    "    temperature_drop=temperature_drop,\n",
    "    \n",
    "    model_type_obs=model_type_obs,\n",
    "    nn_width_obs=nn_width_obs,\n",
    "    nn_depth_obs=nn_depth_obs,\n",
    "    weight_decay_obs=weight_decay_obs,\n",
    "    learning_rate_obs=learning_rate_obs,\n",
    "    \n",
    "    model_type_game=model_type_game,\n",
    "    nn_width_game=nn_width_game,\n",
    "    nn_depth_game=nn_depth_game,\n",
    "    weight_decay_game=weight_decay_game,\n",
    "    learning_rate_game=learning_rate_game,\n",
    "    \n",
    "    omniscient_observer=omniscient_observer,\n",
    "    use_reward_policy=use_reward_policy,\n",
    "    optimism=optimism,\n",
    "    use_teacher_forcing=use_teacher_forcing,\n",
    "    n_previous_observations=n_previous_observations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and game\n",
    "run_name = generate_run_name(f'{LOGS_DIR}/{game_name}', game_prefix, match_arbitrary_suffixes=True)\n",
    "print(f\"Starting run: {run_name}\")\n",
    "\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "# Setup Model Manager\n",
    "observation_model_config = OpenSpielModelConfig(\n",
    "                           game, \n",
    "                           model_type_obs, \n",
    "                           [game.observation_tensor_shape()[0] * n_previous_observations], \n",
    "                           nn_width_obs, \n",
    "                           nn_depth_obs, \n",
    "                           weight_decay_obs, \n",
    "                           learning_rate_obs,\n",
    "                           omniscient_observer=False, output_shape=len(state_to_value))\n",
    "observation_model_manager = OpenSpielCheckpointManager(game_name, f\"{run_name}-observation_model\")\n",
    "observation_model_manager.store_config(observation_model_config)\n",
    "\n",
    "\n",
    "game_model_config = OpenSpielModelConfig(\n",
    "                           game, \n",
    "                           model_type_game, \n",
    "                           get_observation_tensor_shape(game, omniscient_observer), \n",
    "                           nn_width_game, \n",
    "                           nn_depth_game, \n",
    "                           weight_decay_game, \n",
    "                           learning_rate_game,\n",
    "                           omniscient_observer=omniscient_observer)\n",
    "game_model_manager = OpenSpielCheckpointManager(game_name, f\"{run_name}-game_model\")\n",
    "game_model_manager.store_config(game_model_config)\n",
    "\n",
    "model_manager = {\"game_model_manager\": game_model_manager, \"observation_model_manager\": observation_model_manager}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Evaluation Manager\n",
    "evaluation_manager = EvaluationManager(game, n_evaluations, evaluation_mcts_config)\n",
    "\n",
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "rating_systems = [elo_rating_system, true_skill_rating_system]\n",
    "\n",
    "# Setup final training manager\n",
    "train_manager = AlphaOneTrainManager(game, model_manager, evaluation_manager, n_most_recent_train_samples, n_most_recent_valid_samples, rating_systems)\n",
    "\n",
    "print(\"Observation Model: Num variables:\", train_manager.observation_model_challenger.num_trainable_variables)\n",
    "train_manager.observation_model_challenger.print_trainable_variables()\n",
    "print(\"\")\n",
    "print(\"Game Model: Num variables:\", train_manager.game_model_challenger.num_trainable_variables)\n",
    "train_manager.game_model_challenger.print_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}-observation_model\")\n",
    "observation_tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}-game_model\")\n",
    "game_tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, n_iterations + 1):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_observation_samples,\\\n",
    "    new_valid_observation_samples,\\\n",
    "    new_train_game_samples,\\\n",
    "    new_valid_game_samples = train_manager.generate_training_data(n_games_train, n_games_valid, mcts_config, use_teacher_forcing=use_teacher_forcing)\n",
    "    print(f'  - Generated {len(new_train_observation_samples)} additional training observation samples and {len(new_valid_observation_samples)} additional validation observation samples')\n",
    "    print(f'  - Generated {len(new_train_game_samples)} additional training game samples and {len(new_valid_game_samples)} additional validation game samples')\n",
    "    observation_tensorboard.log_scalar(\"n_training_observation_samples\", train_manager.replay_buffer_observation.get_total_samples(), iteration)\n",
    "    game_tensorboard.log_scalar(\"n_training_game_samples\", train_manager.replay_buffer_model.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    train_observation_losses,\\\n",
    "    valid_observation_losses,\\\n",
    "    train_game_losses,\\\n",
    "    valid_game_losses = train_manager.train_model(n_train_steps_obs, n_train_steps_game, n_valid_steps, batch_size, weight_decay_obs, weight_decay_game)\n",
    "    print(f'  - Training Observation Model: {mean_total_loss(train_observation_losses[:int(len(train_observation_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_observation_losses[int(len(train_observation_losses)/4):int(2 * len(train_observation_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_observation_losses[int(2 * len(train_observation_losses)/4):int(3 * len(train_observation_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_observation_losses[int(3 * len(train_observation_losses)/4):]):.2f}')\n",
    "    \n",
    "    print(f'  - Training Game Model: {mean_total_loss(train_game_losses[:int(len(train_game_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_game_losses[int(len(train_game_losses)/4):int(2 * len(train_game_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_game_losses[int(2 * len(train_game_losses)/4):int(3 * len(train_game_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_game_losses[int(3 * len(train_game_losses)/4):]):.2f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    observation_tensorboard.log_scalars(\"Loss\", {\n",
    "        \"total/train\": mean([loss.total for loss in train_observation_losses]),\n",
    "        \"policy/train\": mean([loss.policy for loss in train_observation_losses]),\n",
    "        \"value/train\": mean([loss.value for loss in train_observation_losses]),\n",
    "        \"total/valid\": mean([loss.total for loss in valid_observation_losses]),\n",
    "        \"policy/valid\": mean([loss.policy for loss in valid_observation_losses]),\n",
    "        \"value/valid\": mean([loss.value for loss in valid_observation_losses])\n",
    "    }, iteration)\n",
    "    \n",
    "    \n",
    "    game_tensorboard.log_scalars(\"Loss\", {\n",
    "        \"total/train\": mean([loss.total for loss in train_game_losses]),\n",
    "        \"policy/train\": mean([loss.policy for loss in train_game_losses]),\n",
    "        \"value/train\": mean([loss.value for loss in train_game_losses]),\n",
    "        \"total/valid\": mean([loss.total for loss in valid_game_losses]),\n",
    "        \"policy/valid\": mean([loss.policy for loss in valid_game_losses]),\n",
    "        \"value/valid\": mean([loss.value for loss in valid_game_losses])\n",
    "    }, iteration)\n",
    "    \n",
    "    \n",
    "    challenger_win_rate, challenger_policies, match_outcomes, challenger_average_reward = train_manager.evaluate_challenger_model()\n",
    "    \n",
    "    player_name_current_best = train_manager.get_player_name_current_best()\n",
    "    player_name_challenger = train_manager.get_player_name_challenger()\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    \n",
    "    game_tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    observation_tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    \n",
    "    game_tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    observation_tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    game_tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    observation_tensorboard.log_scalar(\"challenger_average_reward\", challenger_average_reward, iteration)\n",
    "    \n",
    "    observation_tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    train_manager.replace_model_with_challenger(challenger_win_rate, win_ratio_needed)\n",
    "    if challenger_win_rate > win_ratio_needed:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        \n",
    "        \n",
    "    game_tensorboard.flush()\n",
    "    observation_tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally, Train Observation Model a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(100):\n",
    "    batch = train_manager.replay_buffer_observation.sample(100, n_most_recent=100000)\n",
    "    loss = train_manager.observation_model_challenger.update(batch)\n",
    "    losses.append(loss)\n",
    "print(mean([loss.policy for loss in losses]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of Game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_machine = GameMachine(game)\n",
    "game_machine.new_game()\n",
    "game_machine.play_action(0)\n",
    "game_machine.play_action(1)\n",
    "game_machine.play_action(2)\n",
    "game_machine.play_action(2)\n",
    "game_machine.play_action(1)\n",
    "game_machine.play_action(5)\n",
    "\n",
    "guess_state_mask = np.zeros(len(state_to_value), dtype=np.bool)\n",
    "for s in game_machine.information_set_generator.calculate_information_set():\n",
    "    guess_state_mask[state_to_value[s.__str__()]] = 1\n",
    "value, policy = train_manager.observation_model_challenger.inference([game_machine.state.observation_tensor()], [guess_state_mask])\n",
    "policy[0][guess_state_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
