{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "\n",
    "import pyspiel\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from open_spiel.python import policy\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms import policy_gradient\n",
    "\n",
    "from open_spiel.python.algorithms import mcts\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "\n",
    "from alpha_one.model.model_manager import OpenSpielCheckpointManager, PolicyGradientCheckpointManager, PolicyGradientModelManager, PolicyGradientConfig\n",
    "from alpha_one.utils.mcts import initialize_bot\n",
    "from alpha_one.utils.logging import generate_run_name\n",
    "from env import MODEL_SAVES_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Game and Model Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the game\n",
    "game_name = \"connect_four\"\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "# load our trained model\n",
    "run_name = 'C4-13'\n",
    "checkpoint_manager = OpenSpielCheckpointManager(game_name, run_name)\n",
    "model = checkpoint_manager.load_checkpoint(-1)\n",
    "\n",
    "pg_checkpoint_manager = PolicyGradientModelManager(game_name).new_run()\n",
    "\n",
    "# RL environment configurations for policy gradient\n",
    "num_players = 2\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_configs = [\n",
    "    PolicyGradientConfig(\n",
    "        player_id=player_id,\n",
    "        info_state_size=info_state_size,\n",
    "        num_actions=num_actions,\n",
    "        loss_str=\"qpg\",\n",
    "        hidden_layers_sizes=[50, 50, 50, 50, 50],\n",
    "        batch_size=32,\n",
    "        entropy_cost=0.001,\n",
    "        critic_learning_rate=0.01,\n",
    "        pi_learning_rate=0.01,\n",
    "        num_critic_before_pi=4) \n",
    "    for player_id in [0, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [pg_checkpoint_manager.build_model(config) for config in pg_configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Policy Gradient Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of train episodes for policy gradient \n",
    "num_episodes = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(num_episodes):\n",
    "\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "\n",
    "  # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "        agent.step(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store trained Policy Gradient\n",
    "pg_checkpoint_manager.store_config(pg_configs[0])\n",
    "pg_checkpoint_manager.store_checkpoint(agents[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate Policy Gradient vs trained Alpha Zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare our alphazero model with trained policy bot\n",
    "track_wins = []\n",
    "track_lost = []\n",
    "n_evaluations = 100\n",
    "for j in range(10):\n",
    "    wins = 0\n",
    "    for i in range(n_evaluations):\n",
    "        state = game.new_initial_state()\n",
    "        mcts_bot = initialize_bot(game, model, 2, 25, None, None)\n",
    "\n",
    "        # select the first policy bot to play against alphazero bot\n",
    "        policy_bot = agents[0]\n",
    "        while not state.is_terminal():\n",
    "            actions = []\n",
    "            temperature = 1\n",
    "            temperature_drop = 10\n",
    "\n",
    "            # if current turn is of alphazero bot\n",
    "            if state.current_player() == 0:\n",
    "                root = mcts_bot.mcts_search(state)\n",
    "                policy = np.zeros(game.num_distinct_actions())\n",
    "                for c in root.children:\n",
    "                    policy[c.action] = c.explore_count\n",
    "                policy = policy ** (1 / temperature)\n",
    "                policy /= policy.sum()\n",
    "                if len(actions) >= temperature_drop:\n",
    "                    action = root.best_child().action\n",
    "                else:\n",
    "                    action = np.random.choice(len(policy), p=policy)\n",
    "\n",
    "            # if the turn is of policy bot\n",
    "            else:\n",
    "                action, probs = policy_bot._act(state.observation_tensor(), state.legal_actions())\n",
    "\n",
    "            state.apply_action(action)\n",
    "\n",
    "        # if alphazero bot wins\n",
    "        if (state.returns()[0] == 1):\n",
    "            wins += 1\n",
    "\n",
    "    print(f\"Win Rate of AlphaZero: {wins/n_evaluations * 100}%\")\n",
    "    track_wins.append(wins/n_evaluations * 100)\n",
    "    track_lost.append((n_evaluations - wins)/n_evaluations * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "w = 0.4\n",
    "bar1 = np.arange(1, 10 + 1)\n",
    "bar2 = [i + w for i in bar1]\n",
    "plt.bar(bar1, track_wins, w, label=\"AlphaZero\")\n",
    "plt.bar(bar2, track_lost, w, label=\"Policy Gradient\")\n",
    "plt.xticks(bar1+w/2, bar1)\n",
    "plt.xlabel(\"Runs - Each run represents evaluation out of 100 games\")\n",
    "plt.ylabel(\"Win Rate in %\")\n",
    "plt.title(\"Head to Head - AlphaZero vs Policy Gradient\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
