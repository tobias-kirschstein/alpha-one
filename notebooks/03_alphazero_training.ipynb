{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saves_path = '../../model_saves/connect_four'\n",
    "\n",
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "n_selfplay_simulations = 25            # How many play throughs should be generated by best model for training. (Training set size)\n",
    "n_train_steps = 500                    # After how many gradient updates the new model tries to beat the current best\n",
    "n_iterations = 100                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "n_evaluations = 20                     # How many games should be played to measure which model is better\n",
    "batch_size = 256\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "n_most_recent_train_samples = 5000     # Among which training samples to choose to train current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_width = 128\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the tensorflow model\n",
    "def build_model(game, model_type):\n",
    "    return model_lib.Model.build_model(\n",
    "      model_type, game.observation_tensor_shape(), game.num_distinct_actions(),\n",
    "      nn_width=nn_width, nn_depth=nn_depth, weight_decay=weight_decay, learning_rate=learning_rate, path=model_saves_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeEpisode(game, temperature):\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    train_inputs = []\n",
    "    state = game.new_initial_state()\n",
    "    \n",
    "    mcts_bot = mcts.MCTSBot(\n",
    "      game,\n",
    "      UCT_C,\n",
    "      max_simulations=max_mcts_simulations,\n",
    "      solve=True,\n",
    "      random_state=rng,\n",
    "      evaluator=evaluator_lib.AlphaZeroEvaluator(game, model_current_best))\n",
    "    \n",
    "    observations = []\n",
    "    action_masks = []\n",
    "    policies = []\n",
    "        \n",
    "    while not state.is_terminal():\n",
    "        root = mcts_bot.mcts_search(state)\n",
    "        policy = np.zeros(game.num_distinct_actions())\n",
    "        \n",
    "        for c in root.children:\n",
    "            policy[c.action] = c.explore_count\n",
    "        policy = policy ** (1 / temperature)\n",
    "        policy /= policy.sum()\n",
    "        action = np.random.choice(len(policy), p=policy)\n",
    "        obs = state.observation_tensor()\n",
    "        act_mask = state.legal_actions_mask()\n",
    "        \n",
    "        observations.append(obs)\n",
    "        action_masks.append(act_mask)\n",
    "        policies.append(policy)\n",
    "    \n",
    "        # train_inputs.append(model_lib.TrainInput(obs, act_mask, policy, value=1))              \n",
    "   \n",
    "        state.apply_action(action) \n",
    "    \n",
    "    final_game_reward = state.player_reward(0)\n",
    "    train_inputs = [model_lib.TrainInput(obs, act_mask, policy, value=final_game_reward) for obs, act_mask, policy in zip(observations, action_masks, policies)]\n",
    "    \n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    train_inputs = []\n",
    "    for _ in range(n_selfplay_simulations):\n",
    "        train_inputs.extend(executeEpisode(game, 1))\n",
    "    print(f'  - Generated {len(train_inputs)} additional training samples')\n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_inputs):\n",
    "    losses = []   \n",
    "    for _ in range(n_train_steps): \n",
    "        train_set_idx = np.random.choice(range(len(train_inputs)), batch_size)\n",
    "        loss = model.update([train_inputs[i] for i in train_set_idx])\n",
    "        losses.append(loss)\n",
    "    print(f'  - Training: {mean_total_loss(losses[:int(len(losses)/4)]):.2f} \\\n",
    "                -> {mean_total_loss(losses[int(len(losses)/4):int(2 * len(losses)/4)]):.2f} \\\n",
    "                -> {mean_total_loss(losses[int(2 * len(losses)/4):int(3 * len(losses)/4)]):.2f} \\\n",
    "                -> {mean_total_loss(losses[int(3 * len(losses)/4):]):.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_challenger_model():\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    if evaluation_strategy == 'mcts':\n",
    "    \n",
    "        mcts_bot_best_model = mcts.MCTSBot(\n",
    "          game,\n",
    "          UCT_C,\n",
    "          max_simulations=max_mcts_simulations,\n",
    "          solve=True,\n",
    "          random_state=rng,\n",
    "          evaluator=evaluator_lib.AlphaZeroEvaluator(game, model_current_best))\n",
    "\n",
    "        mcts_bot_challenger = mcts.MCTSBot(\n",
    "          game,\n",
    "          UCT_C,\n",
    "          max_simulations=100,\n",
    "          solve=True,\n",
    "          random_state=rng,\n",
    "          evaluator=evaluator_lib.AlphaZeroEvaluator(game, model))\n",
    "    \n",
    "    challenger_results = []\n",
    "    for _ in range(n_evaluations):\n",
    "        model_challenger_player = np.random.choice([0, 1]) # ensure that each model will play as each player\n",
    "        state = game.new_initial_state()\n",
    "        while not state.is_terminal():\n",
    "            model_current_turn = model if state.current_player() == model_challenger_player else model_current_best\n",
    "            \n",
    "            if evaluation_strategy == 'mcts':\n",
    "                mcts_bot_current_turn = mcts_bot_challenger if state.current_player() == model_challenger_player else mcts_bot_best_model\n",
    "                root = mcts_bot_current_turn.mcts_search(state)\n",
    "                policy = compute_mcts_policy(root, 0) # Always choose action with highest visit count\n",
    "            else:\n",
    "                pass\n",
    "                # obs = state.observation_tensor()\n",
    "                # act_mask = state.legal_actions_mask()\n",
    "                # value, policy = model_current_turn.inference([obs], [act_mask])\n",
    "                # TODO: implement\n",
    "            action = policy.argmax(-1)\n",
    "            state.apply_action(action)\n",
    "        challenger_reward = state.player_reward(model_challenger_player)\n",
    "        challenger_results.append(challenger_reward)\n",
    "    \n",
    "    n_challenger_wins = (np.array(challenger_results) == 1).sum()\n",
    "    challenger_win_rate = n_challenger_wins / n_evaluations\n",
    "    print(f'  - Challenger won {n_challenger_wins}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    return challenger_win_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcts_policy(root, temperature):\n",
    "    policy = np.zeros(game.num_distinct_actions())\n",
    "        \n",
    "    for c in root.children:\n",
    "        policy[c.action] = c.explore_count\n",
    "    if temperature == 0 or temperature is None:\n",
    "        # Create probability distribution with peak at most likely action\n",
    "        new_policy = np.zeros(game.num_distinct_actions())\n",
    "        new_policy[policy.argmax(-1)] = 1\n",
    "        policy = new_policy\n",
    "    else:\n",
    "        policy = policy ** (1 / temperature)\n",
    "        policy /= policy.sum()\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])\n",
    "\n",
    "def load_model(iteration):\n",
    "    new_model = build_model(game, 'mlp')\n",
    "    new_model.load_checkpoint(f\"{model._path}/checkpoint-{iteration}\")\n",
    "    return new_model\n",
    "\n",
    "def copy_and_create_checkpoint(iteration):\n",
    "    # Generate checkpoint\n",
    "    model.save_checkpoint(iteration)\n",
    "    return load_model(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num variables: 99848\n",
      "torso_0_dense/kernel:0: (126, 128)\n",
      "torso_0_dense/bias:0: (128,)\n",
      "torso_1_dense/kernel:0: (128, 128)\n",
      "torso_1_dense/bias:0: (128,)\n",
      "torso_2_dense/kernel:0: (128, 128)\n",
      "torso_2_dense/bias:0: (128,)\n",
      "torso_3_dense/kernel:0: (128, 128)\n",
      "torso_3_dense/bias:0: (128,)\n",
      "policy_dense/kernel:0: (128, 128)\n",
      "policy_dense/bias:0: (128,)\n",
      "policy/kernel:0: (128, 7)\n",
      "policy/bias:0: (7,)\n",
      "value_dense/kernel:0: (128, 128)\n",
      "value_dense/bias:0: (128,)\n",
      "value/kernel:0: (128, 1)\n",
      "value/bias:0: (1,)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "game = pyspiel.load_game(\"connect_four\")\n",
    "model = build_model(game, 'mlp')\n",
    "print(\"Num variables:\", model.num_trainable_variables)\n",
    "model.print_trainable_variables()\n",
    "model_current_best = copy_and_create_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  - Generated 564 additional training samples\n",
      "  - Training: 2.24                 -> 1.92                 -> 1.86                 -> 1.82\n",
      "  - Challenger won 4/20 games (20.00% win rate)\n",
      "Iteration 1\n",
      "  - Generated 599 additional training samples\n",
      "  - Training: 2.14                 -> 1.92                 -> 1.88                 -> 1.86\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 2\n",
      "  - Generated 528 additional training samples\n",
      "  - Training: 2.00                 -> 1.89                 -> 1.87                 -> 1.85\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 3\n",
      "  - Generated 683 additional training samples\n",
      "  - Training: 1.97                 -> 1.87                 -> 1.86                 -> 1.84\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 4\n",
      "  - Generated 609 additional training samples\n",
      "  - Training: 1.96                 -> 1.87                 -> 1.86                 -> 1.85\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 5\n",
      "  - Generated 615 additional training samples\n",
      "  - Training: 1.95                 -> 1.88                 -> 1.86                 -> 1.85\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 6\n",
      "  - Generated 545 additional training samples\n",
      "  - Training: 1.95                 -> 1.89                 -> 1.88                 -> 1.87\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 7\n",
      "  - Generated 607 additional training samples\n",
      "  - Training: 1.97                 -> 1.91                 -> 1.88                 -> 1.87\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 8\n",
      "  - Generated 652 additional training samples\n",
      "  - Training: 1.99                 -> 1.91                 -> 1.89                 -> 1.87\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 9\n",
      "  - Generated 565 additional training samples\n",
      "  - Training: 1.99                 -> 1.92                 -> 1.89                 -> 1.88\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 10\n",
      "  - Generated 633 additional training samples\n",
      "  - Training: 2.00                 -> 1.91                 -> 1.89                 -> 1.88\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 11\n",
      "  - Generated 628 additional training samples\n",
      "  - Training: 1.98                 -> 1.92                 -> 1.89                 -> 1.88\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 12\n",
      "  - Generated 596 additional training samples\n",
      "  - Training: 2.01                 -> 1.94                 -> 1.91                 -> 1.90\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 13\n",
      "  - Generated 580 additional training samples\n",
      "  - Training: 2.01                 -> 1.93                 -> 1.90                 -> 1.89\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 14\n",
      "  - Generated 603 additional training samples\n",
      "  - Training: 2.01                 -> 1.91                 -> 1.90                 -> 1.88\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 15\n",
      "  - Generated 586 additional training samples\n",
      "  - Training: 2.02                 -> 1.92                 -> 1.91                 -> 1.90\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 16\n",
      "  - Generated 583 additional training samples\n",
      "  - Training: 2.02                 -> 1.94                 -> 1.91                 -> 1.90\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 17\n",
      "  - Generated 561 additional training samples\n",
      "  - Training: 2.02                 -> 1.95                 -> 1.93                 -> 1.92\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 18\n",
      "  - Generated 608 additional training samples\n",
      "  - Training: 2.02                 -> 1.93                 -> 1.91                 -> 1.91\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 19\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    # 1 Generate training data with current best model\n",
    "    train_inputs.extend(generate_training_data())\n",
    "    train_inputs = train_inputs[-n_most_recent_train_samples:]\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    train_model(train_inputs)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate = evaluate_challenger_model()\n",
    "    if challenger_win_rate > 0.55:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        model_current_best = copy_and_create_checkpoint(iteration)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...o...\n",
      "...xx..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(4)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.8906451]], dtype=float32),\n",
       " array([[0.098325  , 0.155745  , 0.15468678, 0.13765964, 0.09293988,\n",
       "         0.12673675, 0.23390703]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.9152308]], dtype=float32),\n",
       " array([[0.11774666, 0.13667256, 0.16464972, 0.14207387, 0.12070882,\n",
       "         0.13404773, 0.18410066]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.05155722]], dtype=float32),\n",
       " array([[0.13417855, 0.14319465, 0.13845864, 0.13116197, 0.16058978,\n",
       "         0.13832514, 0.15409122]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = load_model(0)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
