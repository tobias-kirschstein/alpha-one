{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'connect_four'\n",
    "game_prefix = 'C4'\n",
    "\n",
    "model_saves_path = f'{MODEL_SAVES_DIR}/{game_name}'\n",
    "tensorboard_log_dir = f'{LOGS_DIR}/{game_name}'\n",
    "\n",
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "\n",
    "n_iterations = 100                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_selfplay_simulations = 10           # How many play throughs should be generated by best model for training. (Training set size)\n",
    "n_new_train_samples = 1000             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_train_steps = 50                     # After how many gradient updates the new model tries to beat the current best\n",
    "batch_size = 256\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 50                     # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = 0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "\n",
    "policy_epsilon = None #0.25                             # What noise epsilon to use\n",
    "policy_alpha = None #1                                  # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'mlp'\n",
    "nn_width = 64\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the tensorflow model\n",
    "def build_model(game):\n",
    "    return model_lib.Model.build_model(\n",
    "      model_type, game.observation_tensor_shape(), game.num_distinct_actions(),\n",
    "      nn_width=nn_width, nn_depth=nn_depth, weight_decay=weight_decay, learning_rate=learning_rate, path=model_saves_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_game(game, bots, temperature, temperature_drop):\n",
    "    trajectory = GameTrajectory()\n",
    "    state = game.new_initial_state()\n",
    "    current_turn = 0\n",
    "    while not state.is_terminal():\n",
    "        root = bots[state.current_player()].mcts_search(state)\n",
    "        \n",
    "        if current_turn < temperature_drop:\n",
    "            policy = compute_mcts_policy(game, root, temperature)\n",
    "        else:\n",
    "            policy = compute_mcts_policy(game, root, 0)\n",
    "        \n",
    "        action = np.random.choice(len(policy), p=policy)\n",
    "        \n",
    "        trajectory.append(state, action, policy)\n",
    "        state.apply_action(action)\n",
    "        current_turn += 1\n",
    "    \n",
    "    trajectory.set_final_rewards(state.returns())\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    n_new_states = 0\n",
    "    train_samples = []\n",
    "    while True:\n",
    "        bot = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        trajectory = play_one_game(game, [bot, bot], temperature, temperature_drop)\n",
    "        p1_outcome = trajectory.get_final_reward(0)\n",
    "        new_train_states = [model_lib.TrainInput(s.observation, s.legals_mask, s.policy, value=p1_outcome) \n",
    "                             for s in trajectory.states]\n",
    "        replay_buffer.extend(new_train_states)\n",
    "        train_samples.extend(new_train_states)\n",
    "        n_new_states += len(trajectory)\n",
    "        if n_new_states > n_new_train_samples:\n",
    "            break\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    losses = []   \n",
    "    for _ in range(n_train_steps): \n",
    "        loss = model.update(replay_buffer.sample(batch_size))\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_challenger_model(model_challenger, model_current_best):\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    challenger_results = []\n",
    "    challenger_policies = []\n",
    "    match_outcomes = []\n",
    "    for _ in range(n_evaluations):\n",
    "        if evaluation_strategy == 'mcts':\n",
    "            mcts_bot_best_model = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "            mcts_bot_challenger = initialize_bot(game, model_challenger, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        \n",
    "        player_id_challenger = np.random.choice([0, 1]) # ensure that each model will play as each player\n",
    "        bots = [mcts_bot_challenger, mcts_bot_best_model] if player_id_challenger == 0 else [mcts_bot_best_model, mcts_bot_challenger]\n",
    "        \n",
    "        trajectory = play_one_game(game, bots, temperature, temperature_drop)\n",
    "        challenger_policies.extend([s.policy for s in trajectory.get_player_states(player_id_challenger)])\n",
    "        \n",
    "        challenger_reward = trajectory.get_final_reward(player_id_challenger)\n",
    "        challenger_results.append(challenger_reward)\n",
    "        match_outcomes.append(\n",
    "            MatchOutcome.win(player_name_challenger, player_name_current_best) \n",
    "            if challenger_reward == 1 else \n",
    "            MatchOutcome.defeat(player_name_challenger, player_name_current_best))\n",
    "    \n",
    "    n_challenger_wins = (np.array(challenger_results) == 1).sum()\n",
    "    challenger_win_rate = n_challenger_wins / n_evaluations\n",
    "    return challenger_win_rate, challenger_policies, match_outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])\n",
    "\n",
    "def load_model(iteration):\n",
    "    new_model = build_model(game)\n",
    "    new_model.load_checkpoint(f\"{model._path}/checkpoint-{iteration}\")\n",
    "    return new_model\n",
    "\n",
    "def copy_and_create_checkpoint(iteration):\n",
    "    # Generate checkpoint\n",
    "    model.save_checkpoint(iteration)\n",
    "    return load_model(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from statistics import mean\n",
    "\n",
    "def calculate_entropy(policies):\n",
    "    return mean([entropy(policy) for policy in policies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and game\n",
    "game = pyspiel.load_game(game_name)\n",
    "model = build_model(game)\n",
    "print(\"Num variables:\", model.num_trainable_variables)\n",
    "model.print_trainable_variables()\n",
    "model_current_best = copy_and_create_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "\n",
    "player_name_current_best = 0\n",
    "player_name_challenger = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = generate_run_name(tensorboard_log_dir, game_prefix)\n",
    "tensorboard = TensorboardLogger(f\"{tensorboard_log_dir}/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "replay_buffer = ReplayBuffer(n_most_recent_train_samples)\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_samples = generate_training_data()\n",
    "    print(f'  - Generated {len(new_train_samples)} additional training samples')\n",
    "    tensorboard.log_scalar(\"n_training_samples\", replay_buffer.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    losses = train_model()\n",
    "    print(f'  - Training: {mean_total_loss(losses[:int(len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(len(losses)/4):int(2 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(2 * len(losses)/4):int(3 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(3 * len(losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalar(\"Loss\", mean_total_loss(losses), iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes = evaluate_challenger_model(model, model_current_best)\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    if challenger_win_rate > win_ratio_needed:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        model_current_best = copy_and_create_checkpoint(iteration)\n",
    "        player_id_current_best = player_name_challenger\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_train_samples])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalars(\"entropy\", {\n",
    "        \"current_best\": label_entropy,\n",
    "        \"challenger\": challenger_entropy}, iteration)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "    \n",
    "    player_name_challenger += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "#state.apply_action(2)\n",
    "#state.apply_action(3)\n",
    "#state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = load_model(0)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
