{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saves_path = '../../model_saves/connect_four'\n",
    "\n",
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "n_selfplay_simulations = 25            # How many play throughs should be generated by best model for training. (Training set size)\n",
    "n_train_steps = 500                    # After how many gradient updates the new model tries to beat the current best\n",
    "n_iterations = 100                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "n_evaluations = 20                     # How many games should be played to measure which model is better\n",
    "batch_size = 256\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "n_most_recent_train_samples = 5000     # Among which training samples to choose to train current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_width = 128\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the tensorflow model\n",
    "def build_model(game, model_type):\n",
    "    return model_lib.Model.build_model(\n",
    "      model_type, game.observation_tensor_shape(), game.num_distinct_actions(),\n",
    "      nn_width=nn_width, nn_depth=nn_depth, weight_decay=weight_decay, learning_rate=learning_rate, path=model_saves_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeEpisode(game, temperature):\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    train_inputs = []\n",
    "    state = game.new_initial_state()\n",
    "    \n",
    "    mcts_bot = mcts.MCTSBot(\n",
    "      game,\n",
    "      UCT_C,\n",
    "      max_simulations=max_mcts_simulations,\n",
    "      solve=True,\n",
    "      random_state=rng,\n",
    "      evaluator=evaluator_lib.AlphaZeroEvaluator(game, model_current_best))\n",
    "    \n",
    "    observations = []\n",
    "    action_masks = []\n",
    "    policies = []\n",
    "        \n",
    "    while not state.is_terminal():\n",
    "        root = mcts_bot.mcts_search(state)\n",
    "        policy = np.zeros(game.num_distinct_actions())\n",
    "        \n",
    "        for c in root.children:\n",
    "            policy[c.action] = c.explore_count\n",
    "        policy = policy ** (1 / temperature)\n",
    "        policy /= policy.sum()\n",
    "        action = np.random.choice(len(policy), p=policy)\n",
    "        obs = state.observation_tensor()\n",
    "        act_mask = state.legal_actions_mask()\n",
    "        \n",
    "        observations.append(obs)\n",
    "        action_masks.append(act_mask)\n",
    "        policies.append(policy)\n",
    "    \n",
    "        # train_inputs.append(model_lib.TrainInput(obs, act_mask, policy, value=1))              \n",
    "   \n",
    "        state.apply_action(action) \n",
    "    \n",
    "    final_game_reward = state.player_reward(0)\n",
    "    train_inputs = [model_lib.TrainInput(obs, act_mask, policy, value=final_game_reward) for obs, act_mask, policy in zip(observations, action_masks, policies)]\n",
    "    \n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    train_inputs = []\n",
    "    for _ in range(n_selfplay_simulations):\n",
    "        train_inputs.extend(executeEpisode(game, 1))\n",
    "    print(f'  - Generated {len(train_inputs)} additional training samples')\n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_inputs):\n",
    "    losses = []   \n",
    "    for _ in range(n_train_steps): \n",
    "        train_set_idx = np.random.choice(range(len(train_inputs)), batch_size)\n",
    "        loss = model.update([train_inputs[i] for i in train_set_idx])\n",
    "        losses.append(loss)\n",
    "    print(f'  - Training: {mean_total_loss(losses[:int(len(losses)/4)]):.2f} \\\n",
    "                -> {mean_total_loss(losses[int(len(losses)/4):int(2 * len(losses)/4)]):.2f} \\\n",
    "                -> {mean_total_loss(losses[int(2 * len(losses)/4):int(3 * len(losses)/4)]):.2f} \\\n",
    "                -> {mean_total_loss(losses[int(3 * len(losses)/4):]):.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_challenger_model(model_challenger, model_current_best):\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    if evaluation_strategy == 'mcts':\n",
    "    \n",
    "        mcts_bot_best_model = mcts.MCTSBot(\n",
    "          game,\n",
    "          UCT_C,\n",
    "          max_simulations=max_mcts_simulations,\n",
    "          solve=True,\n",
    "          random_state=rng,\n",
    "          evaluator=evaluator_lib.AlphaZeroEvaluator(game, model_current_best))\n",
    "\n",
    "        mcts_bot_challenger = mcts.MCTSBot(\n",
    "          game,\n",
    "          UCT_C,\n",
    "          max_simulations=100,\n",
    "          solve=True,\n",
    "          random_state=rng,\n",
    "          evaluator=evaluator_lib.AlphaZeroEvaluator(game, model_challenger))\n",
    "    \n",
    "    challenger_results = []\n",
    "    for _ in range(n_evaluations):\n",
    "        model_challenger_player = np.random.choice([0, 1]) # ensure that each model will play as each player\n",
    "        state = game.new_initial_state()\n",
    "        while not state.is_terminal():\n",
    "            # model_current_turn = model if state.current_player() == model_challenger_player else model_current_best\n",
    "            \n",
    "            if evaluation_strategy == 'mcts':\n",
    "                mcts_bot_current_turn = mcts_bot_challenger if state.current_player() == model_challenger_player else mcts_bot_best_model\n",
    "                root = mcts_bot_current_turn.mcts_search(state)\n",
    "                policy = compute_mcts_policy(root, 0) # Always choose action with highest visit count\n",
    "            else:\n",
    "                pass\n",
    "                # obs = state.observation_tensor()\n",
    "                # act_mask = state.legal_actions_mask()\n",
    "                # value, policy = model_current_turn.inference([obs], [act_mask])\n",
    "                # TODO: implement\n",
    "            action = policy.argmax(-1)\n",
    "            state.apply_action(action)\n",
    "        challenger_reward = state.player_reward(model_challenger_player)\n",
    "        challenger_results.append(challenger_reward)\n",
    "    \n",
    "    n_challenger_wins = (np.array(challenger_results) == 1).sum()\n",
    "    challenger_win_rate = n_challenger_wins / n_evaluations\n",
    "    print(f'  - Challenger won {n_challenger_wins}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    return challenger_win_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcts_policy(root, temperature):\n",
    "    policy = np.zeros(game.num_distinct_actions())\n",
    "        \n",
    "    for c in root.children:\n",
    "        policy[c.action] = c.explore_count\n",
    "    if temperature == 0 or temperature is None:\n",
    "        # Create probability distribution with peak at most likely action\n",
    "        new_policy = np.zeros(game.num_distinct_actions())\n",
    "        new_policy[policy.argmax(-1)] = 1\n",
    "        policy = new_policy\n",
    "    else:\n",
    "        policy = policy ** (1 / temperature)\n",
    "        policy /= policy.sum()\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])\n",
    "\n",
    "def load_model(iteration):\n",
    "    new_model = build_model(game, 'mlp')\n",
    "    new_model.load_checkpoint(f\"{model._path}/checkpoint-{iteration}\")\n",
    "    return new_model\n",
    "\n",
    "def copy_and_create_checkpoint(iteration):\n",
    "    # Generate checkpoint\n",
    "    model.save_checkpoint(iteration)\n",
    "    return load_model(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num variables: 99848\n",
      "torso_0_dense/kernel:0: (126, 128)\n",
      "torso_0_dense/bias:0: (128,)\n",
      "torso_1_dense/kernel:0: (128, 128)\n",
      "torso_1_dense/bias:0: (128,)\n",
      "torso_2_dense/kernel:0: (128, 128)\n",
      "torso_2_dense/bias:0: (128,)\n",
      "torso_3_dense/kernel:0: (128, 128)\n",
      "torso_3_dense/bias:0: (128,)\n",
      "policy_dense/kernel:0: (128, 128)\n",
      "policy_dense/bias:0: (128,)\n",
      "policy/kernel:0: (128, 7)\n",
      "policy/bias:0: (7,)\n",
      "value_dense/kernel:0: (128, 128)\n",
      "value_dense/bias:0: (128,)\n",
      "value/kernel:0: (128, 1)\n",
      "value/bias:0: (1,)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "game = pyspiel.load_game(\"connect_four\")\n",
    "model = build_model(game, 'mlp')\n",
    "print(\"Num variables:\", model.num_trainable_variables)\n",
    "model.print_trainable_variables()\n",
    "model_current_best = copy_and_create_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  - Generated 564 additional training samples\n",
      "  - Training: 2.24                 -> 1.92                 -> 1.86                 -> 1.82\n",
      "  - Challenger won 4/20 games (20.00% win rate)\n",
      "Iteration 1\n",
      "  - Generated 599 additional training samples\n",
      "  - Training: 2.14                 -> 1.92                 -> 1.88                 -> 1.86\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 2\n",
      "  - Generated 528 additional training samples\n",
      "  - Training: 2.00                 -> 1.89                 -> 1.87                 -> 1.85\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 3\n",
      "  - Generated 683 additional training samples\n",
      "  - Training: 1.97                 -> 1.87                 -> 1.86                 -> 1.84\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 4\n",
      "  - Generated 609 additional training samples\n",
      "  - Training: 1.96                 -> 1.87                 -> 1.86                 -> 1.85\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 5\n",
      "  - Generated 615 additional training samples\n",
      "  - Training: 1.95                 -> 1.88                 -> 1.86                 -> 1.85\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 6\n",
      "  - Generated 545 additional training samples\n",
      "  - Training: 1.95                 -> 1.89                 -> 1.88                 -> 1.87\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 7\n",
      "  - Generated 607 additional training samples\n",
      "  - Training: 1.97                 -> 1.91                 -> 1.88                 -> 1.87\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 8\n",
      "  - Generated 652 additional training samples\n",
      "  - Training: 1.99                 -> 1.91                 -> 1.89                 -> 1.87\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 9\n",
      "  - Generated 565 additional training samples\n",
      "  - Training: 1.99                 -> 1.92                 -> 1.89                 -> 1.88\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 10\n",
      "  - Generated 633 additional training samples\n",
      "  - Training: 2.00                 -> 1.91                 -> 1.89                 -> 1.88\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 11\n",
      "  - Generated 628 additional training samples\n",
      "  - Training: 1.98                 -> 1.92                 -> 1.89                 -> 1.88\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 12\n",
      "  - Generated 596 additional training samples\n",
      "  - Training: 2.01                 -> 1.94                 -> 1.91                 -> 1.90\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 13\n",
      "  - Generated 580 additional training samples\n",
      "  - Training: 2.01                 -> 1.93                 -> 1.90                 -> 1.89\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 14\n",
      "  - Generated 603 additional training samples\n",
      "  - Training: 2.01                 -> 1.91                 -> 1.90                 -> 1.88\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 15\n",
      "  - Generated 586 additional training samples\n",
      "  - Training: 2.02                 -> 1.92                 -> 1.91                 -> 1.90\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 16\n",
      "  - Generated 583 additional training samples\n",
      "  - Training: 2.02                 -> 1.94                 -> 1.91                 -> 1.90\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 17\n",
      "  - Generated 561 additional training samples\n",
      "  - Training: 2.02                 -> 1.95                 -> 1.93                 -> 1.92\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 18\n",
      "  - Generated 608 additional training samples\n",
      "  - Training: 2.02                 -> 1.93                 -> 1.91                 -> 1.91\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 19\n",
      "  - Generated 579 additional training samples\n",
      "  - Training: 2.02                 -> 1.95                 -> 1.92                 -> 1.91\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 19 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-19\n",
      "Iteration 20\n",
      "  - Generated 604 additional training samples\n",
      "  - Training: 2.00                 -> 1.94                 -> 1.91                 -> 1.89\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 21\n",
      "  - Generated 652 additional training samples\n",
      "  - Training: 2.02                 -> 1.93                 -> 1.92                 -> 1.90\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 22\n",
      "  - Generated 693 additional training samples\n",
      "  - Training: 2.00                 -> 1.91                 -> 1.88                 -> 1.86\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 23\n",
      "  - Generated 623 additional training samples\n",
      "  - Training: 1.96                 -> 1.89                 -> 1.87                 -> 1.85\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 24\n",
      "  - Generated 476 additional training samples\n",
      "  - Training: 1.95                 -> 1.87                 -> 1.85                 -> 1.84\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 25\n",
      "  - Generated 632 additional training samples\n",
      "  - Training: 1.95                 -> 1.87                 -> 1.84                 -> 1.82\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 26\n",
      "  - Generated 725 additional training samples\n",
      "  - Training: 1.93                 -> 1.85                 -> 1.82                 -> 1.80\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 27\n",
      "  - Generated 689 additional training samples\n",
      "  - Training: 1.88                 -> 1.82                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 28\n",
      "  - Generated 550 additional training samples\n",
      "  - Training: 1.85                 -> 1.78                 -> 1.76                 -> 1.75\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 29\n",
      "  - Generated 606 additional training samples\n",
      "  - Training: 1.87                 -> 1.78                 -> 1.77                 -> 1.75\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 30\n",
      "  - Generated 674 additional training samples\n",
      "  - Training: 1.89                 -> 1.78                 -> 1.77                 -> 1.75\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 31\n",
      "  - Generated 587 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.78                 -> 1.76\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 32\n",
      "  - Generated 649 additional training samples\n",
      "  - Training: 1.88                 -> 1.79                 -> 1.76                 -> 1.75\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 33\n",
      "  - Generated 563 additional training samples\n",
      "  - Training: 1.86                 -> 1.79                 -> 1.76                 -> 1.75\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 34\n",
      "  - Generated 509 additional training samples\n",
      "  - Training: 1.86                 -> 1.78                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 35\n",
      "  - Generated 517 additional training samples\n",
      "  - Training: 1.89                 -> 1.82                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 36\n",
      "  - Generated 616 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 4/20 games (20.00% win rate)\n",
      "Iteration 37\n",
      "  - Generated 598 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 38\n",
      "  - Generated 594 additional training samples\n",
      "  - Training: 1.94                 -> 1.85                 -> 1.81                 -> 1.79\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 39\n",
      "  - Generated 620 additional training samples\n",
      "  - Training: 1.92                 -> 1.83                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 40\n",
      "  - Generated 601 additional training samples\n",
      "  - Training: 1.92                 -> 1.82                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 4/20 games (20.00% win rate)\n",
      "Iteration 41\n",
      "  - Generated 542 additional training samples\n",
      "  - Training: 1.92                 -> 1.83                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 42\n",
      "  - Generated 560 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 43\n",
      "  - Generated 607 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.79                 -> 1.78\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 44\n",
      "  - Generated 528 additional training samples\n",
      "  - Training: 1.88                 -> 1.80                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 45\n",
      "  - Generated 635 additional training samples\n",
      "  - Training: 1.95                 -> 1.81                 -> 1.78                 -> 1.76\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 46\n",
      "  - Generated 555 additional training samples\n",
      "  - Training: 1.88                 -> 1.81                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 47\n",
      "  - Generated 549 additional training samples\n",
      "  - Training: 1.89                 -> 1.82                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 48\n",
      "  - Generated 628 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.79                 -> 1.78\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 49\n",
      "  - Generated 669 additional training samples\n",
      "  - Training: 1.91                 -> 1.80                 -> 1.78                 -> 1.76\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 49 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-49\n",
      "Iteration 50\n",
      "  - Generated 567 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 51\n",
      "  - Generated 618 additional training samples\n",
      "  - Training: 1.87                 -> 1.79                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 4/20 games (20.00% win rate)\n",
      "Iteration 52\n",
      "  - Generated 622 additional training samples\n",
      "  - Training: 1.89                 -> 1.80                 -> 1.77                 -> 1.75\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 53\n",
      "  - Generated 610 additional training samples\n",
      "  - Training: 1.87                 -> 1.78                 -> 1.76                 -> 1.75\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 54\n",
      "  - Generated 637 additional training samples\n",
      "  - Training: 1.85                 -> 1.78                 -> 1.76                 -> 1.75\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 55\n",
      "  - Generated 599 additional training samples\n",
      "  - Training: 1.86                 -> 1.77                 -> 1.75                 -> 1.73\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 56\n",
      "  - Generated 621 additional training samples\n",
      "  - Training: 1.86                 -> 1.78                 -> 1.75                 -> 1.74\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 57\n",
      "  - Generated 564 additional training samples\n",
      "  - Training: 1.87                 -> 1.77                 -> 1.75                 -> 1.74\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 58\n",
      "  - Generated 564 additional training samples\n",
      "  - Training: 1.88                 -> 1.79                 -> 1.76                 -> 1.75\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 59\n",
      "  - Generated 546 additional training samples\n",
      "  - Training: 1.88                 -> 1.80                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 59 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-59\n",
      "Iteration 60\n",
      "  - Generated 555 additional training samples\n",
      "  - Training: 1.90                 -> 1.82                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 61\n",
      "  - Generated 544 additional training samples\n",
      "  - Training: 1.87                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 62\n",
      "  - Generated 619 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 63\n",
      "  - Generated 598 additional training samples\n",
      "  - Training: 1.92                 -> 1.83                 -> 1.80                 -> 1.79\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 64\n",
      "  - Generated 608 additional training samples\n",
      "  - Training: 1.91                 -> 1.83                 -> 1.80                 -> 1.80\n",
      "  - Challenger won 13/20 games (65.00% win rate)\n",
      "  - Model at iteration 64 supersedes previous model (65.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-64\n",
      "Iteration 65\n",
      "  - Generated 633 additional training samples\n",
      "  - Training: 1.94                 -> 1.83                 -> 1.81                 -> 1.79\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 66\n",
      "  - Generated 596 additional training samples\n",
      "  - Training: 1.92                 -> 1.82                 -> 1.80                 -> 1.79\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 67\n",
      "  - Generated 583 additional training samples\n",
      "  - Training: 1.90                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 68\n",
      "  - Generated 593 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 69\n",
      "  - Generated 606 additional training samples\n",
      "  - Training: 1.89                 -> 1.80                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 70\n",
      "  - Generated 609 additional training samples\n",
      "  - Training: 1.89                 -> 1.80                 -> 1.78                 -> 1.76\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 71\n",
      "  - Generated 549 additional training samples\n",
      "  - Training: 1.87                 -> 1.80                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 72\n",
      "  - Generated 512 additional training samples\n",
      "  - Training: 1.87                 -> 1.79                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 73\n",
      "  - Generated 590 additional training samples\n",
      "  - Training: 1.89                 -> 1.79                 -> 1.77                 -> 1.76\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 73 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-73\n",
      "Iteration 74\n",
      "  - Generated 610 additional training samples\n",
      "  - Training: 1.93                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 74 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-74\n",
      "Iteration 75\n",
      "  - Generated 637 additional training samples\n",
      "  - Training: 1.89                 -> 1.81                 -> 1.78                 -> 1.78\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 76\n",
      "  - Generated 561 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 7/20 games (35.00% win rate)\n",
      "Iteration 77\n",
      "  - Generated 603 additional training samples\n",
      "  - Training: 1.92                 -> 1.83                 -> 1.81                 -> 1.79\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 78\n",
      "  - Generated 570 additional training samples\n",
      "  - Training: 1.91                 -> 1.83                 -> 1.82                 -> 1.81\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 79\n",
      "  - Generated 598 additional training samples\n",
      "  - Training: 1.92                 -> 1.84                 -> 1.81                 -> 1.79\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 80\n",
      "  - Generated 612 additional training samples\n",
      "  - Training: 1.94                 -> 1.85                 -> 1.82                 -> 1.80\n",
      "  - Challenger won 13/20 games (65.00% win rate)\n",
      "  - Model at iteration 80 supersedes previous model (65.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-80\n",
      "Iteration 81\n",
      "  - Generated 601 additional training samples\n",
      "  - Training: 1.93                 -> 1.83                 -> 1.81                 -> 1.79\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 82\n",
      "  - Generated 643 additional training samples\n",
      "  - Training: 1.95                 -> 1.84                 -> 1.81                 -> 1.80\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 83\n",
      "  - Generated 507 additional training samples\n",
      "  - Training: 1.90                 -> 1.83                 -> 1.80                 -> 1.80\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 84\n",
      "  - Generated 626 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.80                 -> 1.79\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 85\n",
      "  - Generated 714 additional training samples\n",
      "  - Training: 1.92                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 86\n",
      "  - Generated 550 additional training samples\n",
      "  - Training: 1.90                 -> 1.82                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 87\n",
      "  - Generated 657 additional training samples\n",
      "  - Training: 1.92                 -> 1.80                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 88\n",
      "  - Generated 605 additional training samples\n",
      "  - Training: 1.91                 -> 1.80                 -> 1.77                 -> 1.77\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 88 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-88\n",
      "Iteration 89\n",
      "  - Generated 511 additional training samples\n",
      "  - Training: 1.86                 -> 1.79                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 90\n",
      "  - Generated 618 additional training samples\n",
      "  - Training: 1.91                 -> 1.81                 -> 1.79                 -> 1.77\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n",
      "Iteration 91\n",
      "  - Generated 564 additional training samples\n",
      "  - Training: 1.92                 -> 1.82                 -> 1.79                 -> 1.78\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 92\n",
      "  - Generated 659 additional training samples\n",
      "  - Training: 1.89                 -> 1.81                 -> 1.78                 -> 1.77\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 92 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-92\n",
      "Iteration 93\n",
      "  - Generated 603 additional training samples\n",
      "  - Training: 1.91                 -> 1.81                 -> 1.79                 -> 1.78\n",
      "  - Challenger won 9/20 games (45.00% win rate)\n",
      "Iteration 94\n",
      "  - Generated 590 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 8/20 games (40.00% win rate)\n",
      "Iteration 95\n",
      "  - Generated 644 additional training samples\n",
      "  - Training: 1.92                 -> 1.83                 -> 1.80                 -> 1.79\n",
      "  - Challenger won 6/20 games (30.00% win rate)\n",
      "Iteration 96\n",
      "  - Generated 563 additional training samples\n",
      "  - Training: 1.90                 -> 1.83                 -> 1.81                 -> 1.80\n",
      "  - Challenger won 12/20 games (60.00% win rate)\n",
      "  - Model at iteration 96 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-96\n",
      "Iteration 97\n",
      "  - Generated 585 additional training samples\n",
      "  - Training: 1.92                 -> 1.82                 -> 1.81                 -> 1.80\n",
      "  - Challenger won 11/20 games (55.00% win rate)\n",
      "Iteration 98\n",
      "  - Generated 667 additional training samples\n",
      "  - Training: 1.95                 -> 1.83                 -> 1.80                 -> 1.78\n",
      "  - Challenger won 5/20 games (25.00% win rate)\n",
      "Iteration 99\n",
      "  - Generated 632 additional training samples\n",
      "  - Training: 1.91                 -> 1.82                 -> 1.79                 -> 1.79\n",
      "  - Challenger won 10/20 games (50.00% win rate)\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    # 1 Generate training data with current best model\n",
    "    train_inputs.extend(generate_training_data())\n",
    "    train_inputs = train_inputs[-n_most_recent_train_samples:]\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    train_model(train_inputs)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate = evaluate_challenger_model(model, model_current_best)\n",
    "    if challenger_win_rate > 0.55:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        model_current_best = copy_and_create_checkpoint(iteration)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...o...\n",
      "...xx..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(4)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.93504727]], dtype=float32),\n",
       " array([[0.13526161, 0.09571104, 0.2229089 , 0.22437674, 0.08570125,\n",
       "         0.1612992 , 0.07474129]], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.9561274]], dtype=float32),\n",
       " array([[0.18993177, 0.11807724, 0.19278333, 0.16555892, 0.10689081,\n",
       "         0.1458753 , 0.08088259]], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.05155722]], dtype=float32),\n",
       " array([[0.13417855, 0.14319465, 0.13845864, 0.13116197, 0.16058978,\n",
       "         0.13832514, 0.15409122]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = load_model(0)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Challenger won 9/20 games (45.00% win rate)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
