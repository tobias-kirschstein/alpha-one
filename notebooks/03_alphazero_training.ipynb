{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/ownCloud/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/tum-adlr-ws20-9\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import ray\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem, calculate_entropy\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, play_one_game, mcts_inference\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from alpha_one.model.model_manager import OpenSpielModelManager\n",
    "from alpha_one.model.evaluation import EvaluationManager, ParallelEvaluationManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from alpha_one.train import AlphaZeroTrainManager, MCTSConfig\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'connect_four'\n",
    "game_prefix = 'C4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_games_train = 100             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "n_games_valid = 10\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_most_recent_valid_samples = 50000\n",
    "n_train_steps = 50                     # After how many gradient updates the new model tries to beat the current best\n",
    "n_valid_steps = 5\n",
    "batch_size = 256\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 100                     # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = 0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "\n",
    "# MCTS config\n",
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "\n",
    "policy_epsilon = None #0.25            # What noise epsilon to use\n",
    "policy_alpha = None #1                 # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_config = MCTSConfig(UCT_C, max_mcts_simulations, temperature, temperature_drop, policy_epsilon, policy_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "model_type = 'mlp'\n",
    "nn_width = 64\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    game_name=game_name,\n",
    "    UCT_C=UCT_C,\n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    n_iterations=n_iterations,\n",
    "    \n",
    "    n_games_train=n_games_train,\n",
    "    n_games_valid=n_games_valid,\n",
    "    \n",
    "    n_most_recent_train_samples=n_most_recent_train_samples,\n",
    "    n_most_recent_valid_samples=n_most_recent_valid_samples,\n",
    "    n_train_steps=n_train_steps,\n",
    "    n_valid_steps=n_valid_steps,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    n_evaluations=n_evaluations,\n",
    "    win_ratio_needed=win_ratio_needed,\n",
    "    \n",
    "    policy_epsilon=policy_epsilon,\n",
    "    policy_alpha=policy_alpha,\n",
    "    \n",
    "    temperature=temperature,\n",
    "    temperature_drop=temperature_drop,\n",
    "    \n",
    "    model_type=model_type,\n",
    "    nn_width=nn_width,\n",
    "    nn_depth=nn_depth,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-08 21:25:14,457\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.188.23',\n",
       " 'raylet_ip_address': '192.168.188.23',\n",
       " 'redis_address': '192.168.188.23:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-01-08_21-25-11_759585_13841/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-01-08_21-25-11_759585_13841/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-01-08_21-25-11_759585_13841',\n",
       " 'metrics_export_port': 58900,\n",
       " 'node_id': 'd2779ed438e24506e1eb36176772ae26b26812ed'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_cpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: C4-27\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-27/checkpoint-0\n",
      "AlphaZero Train manager will use parallelism\n",
      "Num variables: 29448\n",
      "torso_0_dense/kernel:0: (126, 64)\n",
      "torso_0_dense/bias:0: (64,)\n",
      "torso_1_dense/kernel:0: (64, 64)\n",
      "torso_1_dense/bias:0: (64,)\n",
      "torso_2_dense/kernel:0: (64, 64)\n",
      "torso_2_dense/bias:0: (64,)\n",
      "torso_3_dense/kernel:0: (64, 64)\n",
      "torso_3_dense/bias:0: (64,)\n",
      "policy_dense/kernel:0: (64, 64)\n",
      "policy_dense/bias:0: (64,)\n",
      "policy/kernel:0: (64, 7)\n",
      "policy/bias:0: (7,)\n",
      "value_dense/kernel:0: (64, 64)\n",
      "value_dense/bias:0: (64,)\n",
      "value/kernel:0: (64, 1)\n",
      "value/bias:0: (1,)\n"
     ]
    }
   ],
   "source": [
    "# Setup model and game\n",
    "run_name = generate_run_name(f'{LOGS_DIR}/{game_name}', game_prefix)\n",
    "print(f\"Starting run: {run_name}\")\n",
    "\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "model_config = OpenSpielModelConfig(game, model_type, nn_width, nn_depth, weight_decay, learning_rate)\n",
    "model_manager = OpenSpielModelManager(f\"{game_name}/{run_name}\")\n",
    "model_manager.store_config(model_config)\n",
    "\n",
    "if ray.is_initialized():\n",
    "    evaluation_manager = ParallelEvaluationManager(game, model_manager, n_evaluations, mcts_config)\n",
    "else:\n",
    "    evaluation_manager = EvaluationManager(game, n_evaluations, mcts_config)\n",
    "train_manager = AlphaZeroTrainManager(game, model_manager, evaluation_manager, n_most_recent_train_samples, n_most_recent_valid_samples)\n",
    "\n",
    "print(\"Num variables:\", train_manager.model_challenger.num_trainable_variables)\n",
    "train_manager.model_challenger.print_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}\")\n",
    "tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m WARNING:tensorflow:From /opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m WARNING:tensorflow:From /opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.443946: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.444956: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.445034: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-113SD1T): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.445352: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.451795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.452772: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.452850: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-113SD1T): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.453132: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.460220: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1992000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.461490: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f39c4000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=13942)\u001b[0m 2021-01-08 21:25:20.461575: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.452603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1992000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.453488: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f273c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=13943)\u001b[0m 2021-01-08 21:25:20.453569: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Generated 2510 additional training samples and 2512 additional validation samples\n",
      "  - Training: 2.70             -> 2.65             -> 2.59             -> 2.53\n",
      "  - Ratings current best: trueskill.Rating(mu=23.986, sigma=0.896), -40.283\n",
      "  - Ratings challenger: trueskill.Rating(mu=26.014, sigma=0.896), 40.283\n",
      "  - Challenger won 62/100 games (62.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-27/checkpoint-1\n",
      "  - Model at iteration 1 supersedes previous model (62.00% win rate)\n",
      "  - Challenger entropy: 0.653\n",
      "  - Label entropy: 0.711\n",
      "Iteration 2\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_samples, new_valid_samples = train_manager.generate_training_data(n_games_train, n_games_train, mcts_config)\n",
    "    print(f'  - Generated {len(new_train_samples)} additional training samples and {len(new_valid_samples)} additional validation samples')\n",
    "    tensorboard.log_scalar(\"n_training_samples\", train_manager.replay_buffer.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    train_losses, valid_losses = train_manager.train_model(n_train_steps, n_valid_steps, batch_size, weight_decay)\n",
    "    print(f'  - Training: {mean_total_loss(train_losses[:int(len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(len(train_losses)/4):int(2 * len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(2 * len(train_losses)/4):int(3 * len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(3 * len(train_losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalars(\"Loss\", {\n",
    "        \"total/train\": mean([loss.total for loss in train_losses]),\n",
    "        \"policy/train\": mean([loss.policy for loss in train_losses]),\n",
    "        \"value/train\": mean([loss.value for loss in train_losses]),\n",
    "        \"total/valid\": mean([loss.total for loss in valid_losses]),\n",
    "        \"policy/valid\": mean([loss.policy for loss in valid_losses]),\n",
    "        \"value/valid\": mean([loss.value for loss in valid_losses])\n",
    "    }, iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes = train_manager.evaluate_challenger_model()\n",
    "    \n",
    "    player_name_current_best = train_manager.get_player_name_current_best()\n",
    "    player_name_challenger = train_manager.get_player_name_challenger()\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    \n",
    "    # 4 Replace current best model with challenger model if it is better\n",
    "    train_manager.replace_model_with_challenger(challenger_win_rate, win_ratio_needed, iteration)\n",
    "    if challenger_win_rate > win_ratio_needed:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        true_skill_rating_system.add_player(train_manager.get_player_name_challenger(), true_skill_rating_system.get_rating(player_name_challenger))\n",
    "        elo_rating_system.add_player(train_manager.get_player_name_challenger(), elo_rating_system.get_rating(player_name_challenger))\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_train_samples])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalars(\"entropy\", {\n",
    "        \"current_best\": label_entropy,\n",
    "        \"challenger\": challenger_entropy}, iteration)\n",
    "    tensorboard.log_scalar(\"best_model_generation\", player_name_current_best, iteration)\n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...o...\n",
      "..xx...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "#state.apply_action(2)\n",
    "#state.apply_action(3)\n",
    "#state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.20870192]], dtype=float32),\n",
       " array([[0.00884751, 0.09215064, 0.7334334 , 0.04794201, 0.06036935,\n",
       "         0.00714639, 0.05011073]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_manager.model_challenger.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.5216639]], dtype=float32),\n",
       " array([[0.04359566, 0.02505189, 0.75428474, 0.02382881, 0.14099368,\n",
       "         0.00186914, 0.01037611]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_manager.model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01010101, 0.22222222, 0.72727273, 0.01010101, 0.01010101,\n",
       "       0.01010101, 0.01010101])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts_inference(game, train_manager.model_challenger, state, uct_c=UCT_C, max_simulations=max_mcts_simulations, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-13/checkpoint-427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.5216639]], dtype=float32),\n",
       " array([[0.04359566, 0.02505189, 0.75428474, 0.02382881, 0.14099368,\n",
       "         0.00186914, 0.01037611]], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = model_manager.load_model(427)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
