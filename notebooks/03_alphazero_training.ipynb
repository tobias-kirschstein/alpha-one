{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/ownCloud/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/tum-adlr-ws20-9\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem, calculate_entropy\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, play_one_game\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from alpha_one.model.model_manager import OpenSpielModelManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'connect_four'\n",
    "game_prefix = 'C4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "\n",
    "n_iterations = 100                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_new_train_samples = 1000             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_train_steps = 50                     # After how many gradient updates the new model tries to beat the current best\n",
    "batch_size = 256\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 50                     # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = 0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "\n",
    "policy_epsilon = None #0.25            # What noise epsilon to use\n",
    "policy_alpha = None #1                 # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "model_type = 'mlp'\n",
    "nn_width = 64\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    game_name=game_name,\n",
    "    UCT_C=UCT_C,\n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    n_iterations=n_iterations,\n",
    "    \n",
    "    n_new_train_samples=n_new_train_samples,\n",
    "    \n",
    "    n_most_recent_train_samples=n_most_recent_train_samples,\n",
    "    n_train_steps=n_train_steps,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    n_evaluations=n_evaluations,\n",
    "    win_ratio_needed=win_ratio_needed,\n",
    "    \n",
    "    policy_epsilon=policy_epsilon,\n",
    "    policy_alpha=policy_alpha,\n",
    "    \n",
    "    temperature=temperature,\n",
    "    temperature_drop=temperature_drop,\n",
    "    \n",
    "    model_type=model_type,\n",
    "    nn_width=nn_width,\n",
    "    nn_depth=nn_depth,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    n_new_states = 0\n",
    "    train_samples = []\n",
    "    while True:\n",
    "        bot = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        trajectory = play_one_game(game, [bot, bot], temperature, temperature_drop)\n",
    "        p1_outcome = trajectory.get_final_reward(0)\n",
    "        new_train_states = [model_lib.TrainInput(s.observation, s.legals_mask, s.policy, value=p1_outcome) \n",
    "                             for s in trajectory.states]\n",
    "        replay_buffer.extend(new_train_states)\n",
    "        train_samples.extend(new_train_states)\n",
    "        n_new_states += len(trajectory)\n",
    "        if n_new_states > n_new_train_samples:\n",
    "            break\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    losses = []   \n",
    "    for _ in range(n_train_steps): \n",
    "        loss = model.update(replay_buffer.sample(batch_size))\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_challenger_model(model_challenger, model_current_best):\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    challenger_results = []\n",
    "    challenger_policies = []\n",
    "    match_outcomes = []\n",
    "    for _ in range(n_evaluations):\n",
    "        if evaluation_strategy == 'mcts':\n",
    "            mcts_bot_best_model = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "            mcts_bot_challenger = initialize_bot(game, model_challenger, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        \n",
    "        player_id_challenger = np.random.choice([0, 1]) # ensure that each model will play as each player\n",
    "        bots = [mcts_bot_challenger, mcts_bot_best_model] if player_id_challenger == 0 else [mcts_bot_best_model, mcts_bot_challenger]\n",
    "        \n",
    "        trajectory = play_one_game(game, bots, temperature, temperature_drop)\n",
    "        challenger_policies.extend([s.policy for s in trajectory.get_player_states(player_id_challenger)])\n",
    "        \n",
    "        challenger_reward = trajectory.get_final_reward(player_id_challenger)\n",
    "        challenger_results.append(challenger_reward)\n",
    "        match_outcomes.append(\n",
    "            MatchOutcome.win(player_name_challenger, player_name_current_best) \n",
    "            if challenger_reward == 1 else \n",
    "            MatchOutcome.defeat(player_name_challenger, player_name_current_best))\n",
    "    \n",
    "    n_challenger_wins = (np.array(challenger_results) == 1).sum()\n",
    "    challenger_win_rate = n_challenger_wins / n_evaluations\n",
    "    return challenger_win_rate, challenger_policies, match_outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])\n",
    "\n",
    "def copy_and_create_checkpoint(iteration):\n",
    "    model_manager.store_model(model, iteration)\n",
    "    return model_manager.load_model(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: C4-9\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Num variables: 29448\n",
      "torso_0_dense/kernel:0: (126, 64)\n",
      "torso_0_dense/bias:0: (64,)\n",
      "torso_1_dense/kernel:0: (64, 64)\n",
      "torso_1_dense/bias:0: (64,)\n",
      "torso_2_dense/kernel:0: (64, 64)\n",
      "torso_2_dense/bias:0: (64,)\n",
      "torso_3_dense/kernel:0: (64, 64)\n",
      "torso_3_dense/bias:0: (64,)\n",
      "policy_dense/kernel:0: (64, 64)\n",
      "policy_dense/bias:0: (64,)\n",
      "policy/kernel:0: (64, 7)\n",
      "policy/bias:0: (7,)\n",
      "value_dense/kernel:0: (64, 64)\n",
      "value_dense/bias:0: (64,)\n",
      "value/kernel:0: (64, 1)\n",
      "value/bias:0: (1,)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "# Setup model and game\n",
    "run_name = generate_run_name(f'{LOGS_DIR}/{game_name}', game_prefix)\n",
    "print(f\"Starting run: {run_name}\")\n",
    "\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "model_config = OpenSpielModelConfig(game, model_type, nn_width, nn_depth, weight_decay, learning_rate)\n",
    "model_manager = OpenSpielModelManager(f\"{game_name}/{run_name}\")\n",
    "model_manager.store_config(model_config)\n",
    "model = model_manager.build_model(model_config)\n",
    "print(\"Num variables:\", model.num_trainable_variables)\n",
    "model.print_trainable_variables()\n",
    "model_current_best = copy_and_create_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "\n",
    "player_name_current_best = 0\n",
    "player_name_challenger = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}\")\n",
    "tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  - Generated 1017 additional training samples\n",
      "  - Training: 2.66             -> 2.60             -> 2.53             -> 2.42\n",
      "  - Ratings current best: trueskill.Rating(mu=24.133, sigma=1.231), -33.273\n",
      "  - Ratings challenger: trueskill.Rating(mu=25.867, sigma=1.231), 33.273\n",
      "  - Challenger won 34/50 games (68.00% win rate)\n",
      "  - Model at iteration 0 supersedes previous model (68.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-0\n",
      "  - Challenger entropy: 0.656\n",
      "  - Label entropy: 0.808\n",
      "Iteration 1\n",
      "  - Generated 1015 additional training samples\n",
      "  - Training: 2.41             -> 2.30             -> 2.25             -> 2.20\n",
      "  - Ratings current best: trueskill.Rating(mu=25.863, sigma=0.915), 3.377\n",
      "  - Ratings challenger: trueskill.Rating(mu=25.647, sigma=1.106), 29.896\n",
      "  - Challenger won 24/50 games (48.00% win rate)\n",
      "  - Challenger entropy: 0.654\n",
      "  - Label entropy: 0.701\n",
      "Iteration 2\n",
      "  - Generated 1021 additional training samples\n",
      "  - Training: 2.35             -> 2.22             -> 2.18             -> 2.13\n",
      "  - Ratings current best: trueskill.Rating(mu=26.738, sigma=0.834), 41.836\n",
      "  - Ratings challenger: trueskill.Rating(mu=25.701, sigma=1.104), -38.459\n",
      "  - Challenger won 24/50 games (48.00% win rate)\n",
      "  - Challenger entropy: 0.619\n",
      "  - Label entropy: 0.812\n",
      "Iteration 3\n",
      "  - Generated 1005 additional training samples\n",
      "  - Training: 2.22             -> 2.19             -> 2.11             -> 2.09\n",
      "  - Ratings current best: trueskill.Rating(mu=27.270, sigma=0.808), 61.135\n",
      "  - Ratings challenger: trueskill.Rating(mu=25.694, sigma=1.103), -19.299\n",
      "  - Challenger won 21/50 games (42.00% win rate)\n",
      "  - Challenger entropy: 0.608\n",
      "  - Label entropy: 0.752\n",
      "Iteration 4\n",
      "  - Generated 1013 additional training samples\n",
      "  - Training: 2.21             -> 2.16             -> 2.16             -> 2.10\n",
      "  - Ratings current best: trueskill.Rating(mu=27.312, sigma=0.798), 11.246\n",
      "  - Ratings challenger: trueskill.Rating(mu=27.877, sigma=1.099), 49.889\n",
      "  - Challenger won 27/50 games (54.00% win rate)\n",
      "  - Challenger entropy: 0.546\n",
      "  - Label entropy: 0.813\n",
      "Iteration 5\n",
      "  - Generated 1003 additional training samples\n",
      "  - Training: 2.19             -> 2.18             -> 2.16             -> 2.13\n",
      "  - Ratings current best: trueskill.Rating(mu=27.606, sigma=0.795), 15.239\n",
      "  - Ratings challenger: trueskill.Rating(mu=27.942, sigma=1.096), -3.993\n",
      "  - Challenger won 27/50 games (54.00% win rate)\n",
      "  - Challenger entropy: 0.618\n",
      "  - Label entropy: 0.720\n",
      "Iteration 6\n",
      "  - Generated 1006 additional training samples\n",
      "  - Training: 2.22             -> 2.17             -> 2.14             -> 2.13\n",
      "  - Ratings current best: trueskill.Rating(mu=28.146, sigma=0.799), 17.460\n",
      "  - Ratings challenger: trueskill.Rating(mu=29.060, sigma=1.123), -2.220\n",
      "  - Challenger won 30/50 games (60.00% win rate)\n",
      "  - Model at iteration 6 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-6\n",
      "  - Challenger entropy: 0.643\n",
      "  - Label entropy: 0.730\n",
      "Iteration 7\n",
      "  - Generated 1008 additional training samples\n",
      "  - Training: 2.20             -> 2.18             -> 2.18             -> 2.19\n",
      "  - Ratings current best: trueskill.Rating(mu=28.926, sigma=0.892), -24.893\n",
      "  - Ratings challenger: trueskill.Rating(mu=30.253, sigma=1.106), 22.673\n",
      "  - Challenger won 29/50 games (58.00% win rate)\n",
      "  - Model at iteration 7 supersedes previous model (58.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-7\n",
      "  - Challenger entropy: 0.771\n",
      "  - Label entropy: 0.808\n",
      "Iteration 8\n",
      "  - Generated 1010 additional training samples\n",
      "  - Training: 2.22             -> 2.21             -> 2.15             -> 2.15\n",
      "  - Ratings current best: trueskill.Rating(mu=29.393, sigma=0.890), -57.796\n",
      "  - Ratings challenger: trueskill.Rating(mu=30.077, sigma=1.112), 80.469\n",
      "  - Challenger won 24/50 games (48.00% win rate)\n",
      "  - Challenger entropy: 0.765\n",
      "  - Label entropy: 0.742\n",
      "Iteration 9\n",
      "  - Generated 1004 additional training samples\n",
      "  - Training: 2.21             -> 2.20             -> 2.19             -> 2.18\n",
      "  - Ratings current best: trueskill.Rating(mu=29.828, sigma=0.828), -27.751\n",
      "  - Ratings challenger: trueskill.Rating(mu=28.567, sigma=1.112), -30.045\n",
      "  - Challenger won 22/50 games (44.00% win rate)\n",
      "  - Challenger entropy: 0.717\n",
      "  - Label entropy: 0.778\n",
      "Iteration 10\n",
      "  - Generated 1005 additional training samples\n",
      "  - Training: 2.23             -> 2.20             -> 2.21             -> 2.18\n",
      "  - Ratings current best: trueskill.Rating(mu=29.565, sigma=0.820), -32.828\n",
      "  - Ratings challenger: trueskill.Rating(mu=32.042, sigma=1.158), 5.077\n",
      "  - Challenger won 33/50 games (66.00% win rate)\n",
      "  - Model at iteration 10 supersedes previous model (66.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-10\n",
      "  - Challenger entropy: 0.737\n",
      "  - Label entropy: 0.935\n",
      "Iteration 11\n",
      "  - Generated 1016 additional training samples\n",
      "  - Training: 2.26             -> 2.20             -> 2.21             -> 2.20\n",
      "  - Ratings current best: trueskill.Rating(mu=31.147, sigma=0.903), -68.369\n",
      "  - Ratings challenger: trueskill.Rating(mu=32.717, sigma=1.113), 73.446\n",
      "  - Challenger won 27/50 games (54.00% win rate)\n",
      "  - Challenger entropy: 0.790\n",
      "  - Label entropy: 0.695\n",
      "Iteration 12\n",
      "  - Generated 1001 additional training samples\n",
      "  - Training: 2.20             -> 2.21             -> 2.20             -> 2.18\n",
      "  - Ratings current best: trueskill.Rating(mu=30.683, sigma=0.831), -107.302\n",
      "  - Ratings challenger: trueskill.Rating(mu=32.693, sigma=1.103), 38.933\n",
      "  - Challenger won 30/50 games (60.00% win rate)\n",
      "  - Model at iteration 12 supersedes previous model (60.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-12\n",
      "  - Challenger entropy: 0.699\n",
      "  - Label entropy: 0.599\n",
      "Iteration 13\n",
      "  - Generated 1006 additional training samples\n",
      "  - Training: 2.26             -> 2.19             -> 2.18             -> 2.18\n",
      "  - Ratings current best: trueskill.Rating(mu=32.153, sigma=0.891), 27.911\n",
      "  - Ratings challenger: trueskill.Rating(mu=31.269, sigma=1.120), 11.021\n",
      "  - Challenger won 21/50 games (42.00% win rate)\n",
      "  - Challenger entropy: 0.664\n",
      "  - Label entropy: 0.686\n",
      "Iteration 14\n",
      "  - Generated 1005 additional training samples\n",
      "  - Training: 2.23             -> 2.28             -> 2.17             -> 2.22\n",
      "  - Ratings current best: trueskill.Rating(mu=31.668, sigma=0.824), -34.940\n",
      "  - Ratings challenger: trueskill.Rating(mu=32.834, sigma=1.094), 62.851\n",
      "  - Challenger won 28/50 games (56.00% win rate)\n",
      "  - Model at iteration 14 supersedes previous model (56.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-14\n",
      "  - Challenger entropy: 0.708\n",
      "  - Label entropy: 0.725\n",
      "Iteration 15\n",
      "  - Generated 1038 additional training samples\n",
      "  - Training: 2.26             -> 2.19             -> 2.23             -> 2.22\n",
      "  - Ratings current best: trueskill.Rating(mu=31.632, sigma=0.897), 62.617\n",
      "  - Ratings challenger: trueskill.Rating(mu=30.717, sigma=1.144), 0.234\n",
      "  - Challenger won 20/50 games (40.00% win rate)\n",
      "  - Challenger entropy: 0.577\n",
      "  - Label entropy: 0.626\n",
      "Iteration 16\n",
      "  - Generated 1014 additional training samples\n",
      "  - Training: 2.23             -> 2.26             -> 2.22             -> 2.24\n",
      "  - Ratings current best: trueskill.Rating(mu=31.236, sigma=0.827), 29.171\n",
      "  - Ratings challenger: trueskill.Rating(mu=30.844, sigma=1.099), 33.446\n",
      "  - Challenger won 23/50 games (46.00% win rate)\n",
      "  - Challenger entropy: 0.702\n",
      "  - Label entropy: 0.704\n",
      "Iteration 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9396fc178be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 1 Generate training data with current best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnew_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  - Generated {len(new_train_samples)} additional training samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n_training_samples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0fc8a55acfe8>\u001b[0m in \u001b[0;36mgenerate_training_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_current_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUCT_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_mcts_simulations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mp1_outcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_final_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         new_train_states = [model_lib.TrainInput(s.observation, s.legals_mask, s.policy, value=p1_outcome) \n",
      "\u001b[0;32m/mnt/d/ownCloud/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/tum-adlr-ws20-9/alpha_one/utils/mcts.py\u001b[0m in \u001b[0;36mplay_one_game\u001b[0;34m(game, bots, temperature, temperature_drop)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcts_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtemperature_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/mcts.py\u001b[0m in \u001b[0;36mmcts_search\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSearchNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m       \u001b[0mvisit_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_tree_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mworking_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworking_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/mcts.py\u001b[0m in \u001b[0;36m_apply_tree_policy\u001b[0;34m(self, root, state)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[1;32m    307\u001b[0m     \u001b[0mvisit_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mworking_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mworking_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "replay_buffer = ReplayBuffer(n_most_recent_train_samples)\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_samples = generate_training_data()\n",
    "    print(f'  - Generated {len(new_train_samples)} additional training samples')\n",
    "    tensorboard.log_scalar(\"n_training_samples\", replay_buffer.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    losses = train_model()\n",
    "    print(f'  - Training: {mean_total_loss(losses[:int(len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(len(losses)/4):int(2 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(2 * len(losses)/4):int(3 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(3 * len(losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalar(\"Loss\", mean_total_loss(losses), iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes = evaluate_challenger_model(model, model_current_best)\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    if challenger_win_rate > win_ratio_needed:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        model_current_best = copy_and_create_checkpoint(iteration)\n",
    "        player_name_current_best = player_name_challenger\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_train_samples])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalars(\"entropy\", {\n",
    "        \"current_best\": label_entropy,\n",
    "        \"challenger\": challenger_entropy}, iteration)\n",
    "    tensorboard.log_scalar(\"best_model_generation\", player_name_current_best, iteration)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "    \n",
    "    player_name_challenger += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-14\n"
     ]
    }
   ],
   "source": [
    "model_14 = model_manager.load_model(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...o...\n",
      "..xx...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "#state.apply_action(2)\n",
    "#state.apply_action(3)\n",
    "#state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.35761005]], dtype=float32),\n",
       " array([[0.10948619, 0.12286805, 0.0528888 , 0.17567453, 0.21128164,\n",
       "         0.15961233, 0.16818854]], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.2619777]], dtype=float32),\n",
       " array([[0.1010843 , 0.11485792, 0.05718435, 0.18016054, 0.19611868,\n",
       "         0.16479564, 0.18579859]], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tobias/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/model_saves/connect_four/C4-9/checkpoint-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.2619777]], dtype=float32),\n",
       " array([[0.1010843 , 0.11485792, 0.05718435, 0.18016054, 0.19611868,\n",
       "         0.16479564, 0.18579859]], dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = model_manager.load_model(14)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/utils/lru_cache.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, key, fn)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Take it out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: b'\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b536b923136d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_challenger_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_current_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6730965da506>\u001b[0m in \u001b[0;36mevaluate_challenger_model\u001b[0;34m(model_challenger, model_current_best)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmcts_bot_challenger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts_bot_best_model\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mplayer_id_challenger\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmcts_bot_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts_bot_challenger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mchallenger_policies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_player_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_id_challenger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/ownCloud/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/tum-adlr-ws20-9/alpha_one/utils/mcts.py\u001b[0m in \u001b[0;36mplay_one_game\u001b[0;34m(game, bots, temperature, temperature_drop)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcts_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtemperature_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/mcts.py\u001b[0m in \u001b[0;36mmcts_search\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0msolved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0msolved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/alpha_zero/evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m\"\"\"Returns a value for the given state.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/alpha_zero/evaluator.py\u001b[0m in \u001b[0;36m_inference\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     value, policy = self._cache.make(\n\u001b[0m\u001b[1;32m     71\u001b[0m         cache_key, lambda: self._model.inference(obs, mask))\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/utils/lru_cache.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, key, fn)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_misses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/alpha_zero/evaluator.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     value, policy = self._cache.make(\n\u001b[0;32m---> 71\u001b[0;31m         cache_key, lambda: self._model.inference(obs, mask))\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Unpack batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/open_spiel/open_spiel/python/algorithms/alpha_zero/model.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, observation, legals_mask)\u001b[0m\n\u001b[1;32m    342\u001b[0m     return self._session.run(\n\u001b[1;32m    343\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_softmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         feed_dict={self._input: np.array(observation, dtype=np.float32),\n\u001b[0m\u001b[1;32m    345\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legals_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegals_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                    self._training: False})\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and\n\u001b[0m\u001b[1;32m    725\u001b[0m         (g is None or g.building_function)):\n\u001b[1;32m    726\u001b[0m       raise TypeError(\"Tensor is unhashable. \"\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mexecuting_eagerly_outside_functions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5582\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m     \u001b[0mouter_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_outer_context_and_inner_device_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mouter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5585\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5395\u001b[0m                                             default._device_function_stack)\n\u001b[1;32m   5396\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5397\u001b[0;31m       with super(_DefaultGraphStack,\n\u001b[0m\u001b[1;32m   5398\u001b[0m                  self).get_controller(default) as g, context.graph_mode():\n\u001b[1;32m   5399\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_ = evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
