{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/ownCloud/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/tum-adlr-ws20-9\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem\n",
    "from alpha_one.utils.logging import TensorboardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'connect_four'\n",
    "\n",
    "model_saves_path = f'../model_saves/{game_name}'\n",
    "tensorboard_log_dir = f'../tensorboard-logs/{game_name}'\n",
    "\n",
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "n_selfplay_simulations = 10           # How many play throughs should be generated by best model for training. (Training set size)\n",
    "n_train_steps = 50                     # After how many gradient updates the new model tries to beat the current best\n",
    "n_iterations = 100                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "n_evaluations = 50                     # How many games should be played to measure which model is better\n",
    "batch_size = 256\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "\n",
    "policy_epsilon = None #0.25                             # What noise epsilon to use\n",
    "policy_alpha = None #1                                  # What dirichlet noise alpha to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'mlp'\n",
    "nn_width = 10\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the tensorflow model\n",
    "def build_model(game):\n",
    "    return model_lib.Model.build_model(\n",
    "      model_type, game.observation_tensor_shape(), game.num_distinct_actions(),\n",
    "      nn_width=nn_width, nn_depth=nn_depth, weight_decay=weight_decay, learning_rate=learning_rate, path=model_saves_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bot(game, model, uct_c, max_simulations, policy_epsilon=None, policy_alpha=None):\n",
    "    \n",
    "    if policy_epsilon == None or policy_alpha == None:\n",
    "        noise = None\n",
    "    else:\n",
    "        noise = (policy_epsilon, policy_alpha)\n",
    "\n",
    "    az_evaluator = evaluator_lib.AlphaZeroEvaluator(game, model)\n",
    "\n",
    "    bot = mcts.MCTSBot(\n",
    "          game,\n",
    "          uct_c,\n",
    "          max_simulations,\n",
    "          az_evaluator,\n",
    "          solve=False,\n",
    "          dirichlet_noise=noise,\n",
    "          child_selection_fn=mcts.SearchNode.puct_value,\n",
    "          verbose=False)\n",
    "    \n",
    "    return bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeEpisode(game, temperature):\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    train_inputs = []\n",
    "    state = game.new_initial_state()\n",
    "    \n",
    "    mcts_bot = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "    \n",
    "    observations = []\n",
    "    action_masks = []\n",
    "    policies = []\n",
    "        \n",
    "    while not state.is_terminal():\n",
    "        root = mcts_bot.mcts_search(state)\n",
    "        policy = np.zeros(game.num_distinct_actions())\n",
    "        \n",
    "        for c in root.children:\n",
    "            policy[c.action] = c.explore_count\n",
    "        policy = policy ** (1 / temperature)\n",
    "        policy /= policy.sum()\n",
    "        action = np.random.choice(len(policy), p=policy)\n",
    "        obs = state.observation_tensor()\n",
    "        act_mask = state.legal_actions_mask()\n",
    "        \n",
    "        observations.append(obs)\n",
    "        action_masks.append(act_mask)\n",
    "        policies.append(policy)\n",
    "    \n",
    "        # train_inputs.append(model_lib.TrainInput(obs, act_mask, policy, value=1))              \n",
    "   \n",
    "        state.apply_action(action) \n",
    "    \n",
    "    final_game_reward = state.player_reward(0)\n",
    "    train_inputs = [model_lib.TrainInput(obs, act_mask, policy, value=final_game_reward) for obs, act_mask, policy in zip(observations, action_masks, policies)]\n",
    "    \n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    train_inputs = []\n",
    "    for i in range(n_selfplay_simulations):\n",
    "        train_inputs.extend(executeEpisode(game, 1))\n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_inputs):\n",
    "    losses = []   \n",
    "    for _ in range(n_train_steps): \n",
    "        train_set_idx = np.random.choice(range(len(train_inputs)), batch_size)\n",
    "        loss = model.update([train_inputs[i] for i in train_set_idx])\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_challenger_model(model_challenger, model_current_best):\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    challenger_results = []\n",
    "    challenger_policies = []\n",
    "    match_outcomes = []\n",
    "    for _ in range(n_evaluations):\n",
    "        if evaluation_strategy == 'mcts':\n",
    "            mcts_bot_best_model = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "            mcts_bot_challenger = initialize_bot(game, model_challenger, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        \n",
    "        model_challenger_player = np.random.choice([0, 1]) # ensure that each model will play as each player\n",
    "        state = game.new_initial_state()\n",
    "        current_turn = 0\n",
    "        while not state.is_terminal():\n",
    "            # model_current_turn = model if state.current_player() == model_challenger_player else model_current_best\n",
    "            \n",
    "            if evaluation_strategy == 'mcts':\n",
    "                mcts_bot_current_turn = mcts_bot_challenger if state.current_player() == model_challenger_player else mcts_bot_best_model\n",
    "                root = mcts_bot_current_turn.mcts_search(state)\n",
    "                if current_turn < 30:\n",
    "                    policy = compute_mcts_policy(root, 1) # Choose action proportional to visit count (Exploration)\n",
    "                else:\n",
    "                    policy = compute_mcts_policy(root, 0) # Always choose action with highest visit count\n",
    "            else:\n",
    "                pass\n",
    "                # obs = state.observation_tensor()\n",
    "                # act_mask = state.legal_actions_mask()\n",
    "                # value, policy = model_current_turn.inference([obs], [act_mask])\n",
    "                # TODO: implement\n",
    "            action = np.random.choice(range(len(policy)), p=policy)\n",
    "            state.apply_action(action)\n",
    "            current_turn += 1\n",
    "            if state.current_player() == model_challenger_player:\n",
    "                challenger_policies.append(policy)\n",
    "        \n",
    "        challenger_reward = state.player_reward(model_challenger_player)\n",
    "        challenger_results.append(challenger_reward)\n",
    "        match_outcomes.append(\n",
    "            MatchOutcome.win(player_id_challenger, player_id_current_best) \n",
    "            if challenger_reward == 1 else \n",
    "            MatchOutcome.defeat(player_id_challenger, player_id_current_best))\n",
    "    \n",
    "    n_challenger_wins = (np.array(challenger_results) == 1).sum()\n",
    "    challenger_win_rate = n_challenger_wins / n_evaluations\n",
    "    return challenger_win_rate, challenger_policies, match_outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcts_policy(root, temperature):\n",
    "    policy = np.zeros(game.num_distinct_actions())\n",
    "        \n",
    "    for c in root.children:\n",
    "        policy[c.action] = c.explore_count\n",
    "    if temperature == 0 or temperature is None:\n",
    "        # Create probability distribution with peak at most likely action\n",
    "        new_policy = np.zeros(game.num_distinct_actions())\n",
    "        new_policy[policy.argmax(-1)] = 1\n",
    "        policy = new_policy\n",
    "    else:\n",
    "        policy = policy ** (1 / temperature)\n",
    "        policy /= policy.sum()\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])\n",
    "\n",
    "def load_model(iteration):\n",
    "    new_model = build_model(game)\n",
    "    new_model.load_checkpoint(f\"{model._path}/checkpoint-{iteration}\")\n",
    "    return new_model\n",
    "\n",
    "def copy_and_create_checkpoint(iteration):\n",
    "    # Generate checkpoint\n",
    "    model.save_checkpoint(iteration)\n",
    "    return load_model(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from statistics import mean\n",
    "\n",
    "def calculate_entropy(policies):\n",
    "    return mean([entropy(policy) for policy in policies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Num variables: 1908\n",
      "torso_0_dense/kernel:0: (126, 10)\n",
      "torso_0_dense/bias:0: (10,)\n",
      "torso_1_dense/kernel:0: (10, 10)\n",
      "torso_1_dense/bias:0: (10,)\n",
      "torso_2_dense/kernel:0: (10, 10)\n",
      "torso_2_dense/bias:0: (10,)\n",
      "torso_3_dense/kernel:0: (10, 10)\n",
      "torso_3_dense/bias:0: (10,)\n",
      "policy_dense/kernel:0: (10, 10)\n",
      "policy_dense/bias:0: (10,)\n",
      "policy/kernel:0: (10, 7)\n",
      "policy/bias:0: (7,)\n",
      "value_dense/kernel:0: (10, 10)\n",
      "value_dense/bias:0: (10,)\n",
      "value/kernel:0: (10, 1)\n",
      "value/bias:0: (1,)\n",
      "INFO:tensorflow:Restoring parameters from ../model_saves/connect_four/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "# Setup model and game\n",
    "game = pyspiel.load_game(game_name)\n",
    "model = build_model(game)\n",
    "print(\"Num variables:\", model.num_trainable_variables)\n",
    "model.print_trainable_variables()\n",
    "model_current_best = copy_and_create_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "\n",
    "player_id_current_best = 0\n",
    "player_id_challenger = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tensorboard\n",
    "# TODO: generate unique model name\n",
    "run_name = 'run-1'\n",
    "tensorboard = TensorboardLogger(f\"{tensorboard_log_dir}/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  - Generated 211 additional training samples\n",
      "  - Training: 3.18             -> 2.94             -> 2.87             -> 2.84\n",
      "  - Ratings current best: trueskill.Rating(mu=25.379, sigma=1.155), 29.978\n",
      "  - Ratings challenger: trueskill.Rating(mu=24.621, sigma=1.155), -29.978\n",
      "  - Challenger won 23.0/50 games (46.00% win rate)\n",
      "  - Challenger entropy: 1.654\n",
      "  - Label entropy: 1.683\n",
      "Iteration 1\n",
      "  - Generated 382 additional training samples\n",
      "  - Training: 2.86             -> 2.85             -> 2.85             -> 2.84\n",
      "  - Ratings current best: trueskill.Rating(mu=25.259, sigma=0.899), -8.035\n",
      "  - Ratings challenger: trueskill.Rating(mu=26.520, sigma=1.106), 38.014\n",
      "  - Challenger won 29.0/50 games (58.00% win rate)\n",
      "  - Model at iteration 1 supersedes previous model (58.00% win rate)\n",
      "INFO:tensorflow:Restoring parameters from ../model_saves/connect_four/checkpoint-1\n",
      "  - Challenger entropy: 1.728\n",
      "  - Label entropy: 1.647\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"entropy/current_best/write_summary/Identity:0\", shape=(), dtype=float32, device=/device:CPU:0) must be from the same graph as Tensor(\"create_file_writer/SummaryWriter:0\", shape=(), dtype=resource, device=/device:CPU:0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2bf5a492e44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - Label entropy: {label_entropy:0.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entropy/current_best\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entropy/challenger\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchallenger_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/ownCloud/Uni/Semester Ma 5/Advanced Deep Learning for Robotics (IN2349)/Project/tum-adlr-ws20-9/alpha_one/utils/logging.py\u001b[0m in \u001b[0;36mlog_scalar\u001b[0;34m(self, name, value, step)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorboard/plugins/scalar/summary_v2.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, data, step, description)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msummary_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scalar_summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         return tf.summary.write(\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(tag, tensor, step, metadata, name)\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m     op = smart_cond.smart_cond(\n\u001b[0m\u001b[1;32m    675\u001b[0m         _should_record_summaries_v2(), record, _nothing, name=\"summary_cond\")\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m()\u001b[0m\n\u001b[1;32m    662\u001b[0m         summary_tensor = tensor() if callable(tensor) else array_ops.identity(\n\u001b[1;32m    663\u001b[0m             tensor)\n\u001b[0;32m--> 664\u001b[0;31m         write_summary_op = gen_summary_ops.write_summary(\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0m_summary_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_summary\u001b[0;34m(writer, step, tensor, tag, summary_metadata, name)\u001b[0m\n\u001b[1;32m    684\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;34m\"WriteSummary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                         summary_metadata=summary_metadata, name=name)\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5919\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5920\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5921\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5922\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5923\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/alpha_one/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5853\u001b[0m   \"\"\"\n\u001b[1;32m   5854\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5855\u001b[0;31m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[0m\u001b[1;32m   5856\u001b[0m                      (item, original_item))\n\u001b[1;32m   5857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"entropy/current_best/write_summary/Identity:0\", shape=(), dtype=float32, device=/device:CPU:0) must be from the same graph as Tensor(\"create_file_writer/SummaryWriter:0\", shape=(), dtype=resource, device=/device:CPU:0)."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "train_inputs = []\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_training_data = generate_training_data()\n",
    "    train_inputs.extend(new_training_data)\n",
    "    train_inputs = train_inputs[-n_most_recent_train_samples:]\n",
    "    print(f'  - Generated {len(train_inputs)} additional training samples')\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    losses = train_model(train_inputs)\n",
    "    print(f'  - Training: {mean_total_loss(losses[:int(len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(len(losses)/4):int(2 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(2 * len(losses)/4):int(3 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(3 * len(losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalar(\"Loss\", mean_total_loss(losses), iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes = evaluate_challenger_model(model, model_current_best)\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_id_current_best)}, {elo_rating_system.get_rating(player_id_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_id_challenger)}, {elo_rating_system.get_rating(player_id_challenger):0.3f}\")\n",
    "    tensorboard.log_scalar(\"elo_rating/current_best\", elo_rating_system.get_rating(player_id_current_best), iteration)\n",
    "    tensorboard.log_scalar(\"elo_rating/challenger\", elo_rating_system.get_rating(player_id_challenger), iteration)\n",
    "    tensorboard.log_scalar(\"true_skill_rating/current_best\", true_skill_rating_system.get_rating(player_id_current_best).mu, iteration)\n",
    "    tensorboard.log_scalar(\"true_skill_rating/challenger\", true_skill_rating_system.get_rating(player_id_challenger).mu, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    if challenger_win_rate > 0.55:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        model_current_best = copy_and_create_checkpoint(iteration)\n",
    "        player_id_current_best = player_id_challenger\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_training_data])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalar(\"entropy/current_best\", label_entropy, iteration)\n",
    "    tensorboard.log_scalar(\"entropy/challenger\", challenger_entropy, iteration)\n",
    "    \n",
    "    player_id_challenger += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(tensorboard_log_dir)\n",
    "writer.add_scalar(\"hi\", 1)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tensorboard-logs/connect_four'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(tensorboard_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...o...\n",
      "..xx...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "#state.apply_action(2)\n",
    "#state.apply_action(3)\n",
    "#state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.40367833]], dtype=float32),\n",
       " array([[0.09463616, 0.13984184, 0.13587886, 0.15393914, 0.2371599 ,\n",
       "         0.10649913, 0.13204496]], dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.21628237]], dtype=float32),\n",
       " array([[0.09736948, 0.13812093, 0.13371477, 0.15287039, 0.23582101,\n",
       "         0.10691894, 0.13518453]], dtype=float32)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../model_saves/connect_four/checkpoint-0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.00363491]], dtype=float32),\n",
       " array([[0.13665117, 0.14974193, 0.14714976, 0.13916534, 0.14012644,\n",
       "         0.14000261, 0.14716277]], dtype=float32)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = load_model(0)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
