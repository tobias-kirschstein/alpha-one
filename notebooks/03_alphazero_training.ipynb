{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem, calculate_entropy\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, play_one_game\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from alpha_one.model.model_manager import OpenSpielModelManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'connect_four'\n",
    "game_prefix = 'C4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "\n",
    "n_iterations = 100                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_new_train_samples = 1000             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_train_steps = 50                     # After how many gradient updates the new model tries to beat the current best\n",
    "batch_size = 256\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 50                     # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = 0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "\n",
    "policy_epsilon = None #0.25            # What noise epsilon to use\n",
    "policy_alpha = None #1                 # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "model_type = 'mlp'\n",
    "nn_width = 64\n",
    "nn_depth = 4\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    game_name=game_name,\n",
    "    UCT_C=UCT_C,\n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    n_iterations=n_iterations,\n",
    "    \n",
    "    n_new_train_samples=n_new_train_samples,\n",
    "    \n",
    "    n_most_recent_train_samples=n_most_recent_train_samples,\n",
    "    n_train_steps=n_train_steps,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    n_evaluations=n_evaluations,\n",
    "    win_ratio_needed=win_ratio_needed,\n",
    "    \n",
    "    policy_epsilon=policy_epsilon,\n",
    "    policy_alpha=policy_alpha,\n",
    "    \n",
    "    temperature=temperature,\n",
    "    temperature_drop=temperature_drop,\n",
    "    \n",
    "    model_type=model_type,\n",
    "    nn_width=nn_width,\n",
    "    nn_depth=nn_depth,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    n_new_states = 0\n",
    "    train_samples = []\n",
    "    while True:\n",
    "        bot = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        trajectory = play_one_game(game, [bot, bot], temperature, temperature_drop)\n",
    "        p1_outcome = trajectory.get_final_reward(0)\n",
    "        new_train_states = [model_lib.TrainInput(s.observation, s.legals_mask, s.policy, value=p1_outcome) \n",
    "                             for s in trajectory.states]\n",
    "        replay_buffer.extend(new_train_states)\n",
    "        train_samples.extend(new_train_states)\n",
    "        n_new_states += len(trajectory)\n",
    "        if n_new_states > n_new_train_samples:\n",
    "            break\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    losses = []   \n",
    "    for _ in range(n_train_steps): \n",
    "        loss = model.update(replay_buffer.sample(batch_size))\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_challenger_model(model_challenger, model_current_best):\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    challenger_results = []\n",
    "    challenger_policies = []\n",
    "    match_outcomes = []\n",
    "    for _ in range(n_evaluations):\n",
    "        if evaluation_strategy == 'mcts':\n",
    "            mcts_bot_best_model = initialize_bot(game, model_current_best, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "            mcts_bot_challenger = initialize_bot(game, model_challenger, UCT_C, max_mcts_simulations, policy_epsilon, policy_alpha)\n",
    "        \n",
    "        player_id_challenger = np.random.choice([0, 1]) # ensure that each model will play as each player\n",
    "        bots = [mcts_bot_challenger, mcts_bot_best_model] if player_id_challenger == 0 else [mcts_bot_best_model, mcts_bot_challenger]\n",
    "        \n",
    "        trajectory = play_one_game(game, bots, temperature, temperature_drop)\n",
    "        challenger_policies.extend([s.policy for s in trajectory.get_player_states(player_id_challenger)])\n",
    "        \n",
    "        challenger_reward = trajectory.get_final_reward(player_id_challenger)\n",
    "        challenger_results.append(challenger_reward)\n",
    "        match_outcomes.append(\n",
    "            MatchOutcome.win(player_name_challenger, player_name_current_best) \n",
    "            if challenger_reward == 1 else \n",
    "            MatchOutcome.defeat(player_name_challenger, player_name_current_best))\n",
    "    \n",
    "    n_challenger_wins = (np.array(challenger_results) == 1).sum()\n",
    "    challenger_win_rate = n_challenger_wins / n_evaluations\n",
    "    return challenger_win_rate, challenger_policies, match_outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])\n",
    "\n",
    "def copy_and_create_checkpoint(iteration):\n",
    "    model_manager.store_model(model, iteration)\n",
    "    return model_manager.load_model(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and game\n",
    "run_name = generate_run_name(f'{LOGS_DIR}/{game_name}', game_prefix)\n",
    "print(f\"Starting run: {run_name}\")\n",
    "\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "model_config = OpenSpielModelConfig(game, model_type, nn_width, nn_depth, weight_decay, learning_rate)\n",
    "model_manager = OpenSpielModelManager(f\"{game_name}/{run_name}\")\n",
    "model_manager.store_config(model_config)\n",
    "model = model_manager.build_model(model_config)\n",
    "print(\"Num variables:\", model.num_trainable_variables)\n",
    "model.print_trainable_variables()\n",
    "model_current_best = copy_and_create_checkpoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "\n",
    "player_name_current_best = 0\n",
    "player_name_challenger = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}\")\n",
    "tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "replay_buffer = ReplayBuffer(n_most_recent_train_samples)\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_samples = generate_training_data()\n",
    "    print(f'  - Generated {len(new_train_samples)} additional training samples')\n",
    "    tensorboard.log_scalar(\"n_training_samples\", replay_buffer.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    losses = train_model()\n",
    "    print(f'  - Training: {mean_total_loss(losses[:int(len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(len(losses)/4):int(2 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(2 * len(losses)/4):int(3 * len(losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(losses[int(3 * len(losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalar(\"Loss\", mean_total_loss(losses), iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes = evaluate_challenger_model(model, model_current_best)\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    if challenger_win_rate > win_ratio_needed:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        model_current_best = copy_and_create_checkpoint(iteration)\n",
    "        player_name_current_best = player_name_challenger\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_train_samples])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalars(\"entropy\", {\n",
    "        \"current_best\": label_entropy,\n",
    "        \"challenger\": challenger_entropy}, iteration)\n",
    "    tensorboard.log_scalar(\"best_model_generation\", player_name_current_best, iteration)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "    \n",
    "    player_name_challenger += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "#state.apply_action(2)\n",
    "#state.apply_action(3)\n",
    "#state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = model_manager.load_model(14)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one]",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
