{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import ray\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem, calculate_entropy\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.game.observer import get_observation_tensor_shape\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, play_one_game, mcts_inference\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from alpha_one.model.model_manager import OpenSpielCheckpointManager, OpenSpielModelManager\n",
    "from alpha_one.model.evaluation import EvaluationManager, ParallelEvaluationManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from alpha_one.train import AlphaZeroTrainManager, MCTSConfig\n",
    "from alpha_one.data.replay import ReplayDataManager\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt at imitating the training procedure of AlphaZero. It is comprised of 3 main parts:  \n",
    " 1. Generating training data using MCTS and the current best model\n",
    " 2. Updating weights of a challenger model using the generated training data\n",
    " 3. Evaluating the challenger model against the current best model. If it can beat it by a significant margin, the challenger model will from then on be used for generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'leduc_poker'\n",
    "game_prefix = 'LP-local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_games_train = 100             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "n_games_valid = 10\n",
    "store_replays_every = 10\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_most_recent_valid_samples = 50000\n",
    "n_train_steps = 40                     # After how many gradient updates the new model tries to beat the current best\n",
    "n_valid_steps = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 100                    # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = None #0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "average_reward_needed = 0.2            # Minimum average reward over current best model that the challenger model needs in order to supersede the current best model. Mutually exclusive with win_ratio_needed \n",
    "\n",
    "# MCTS config\n",
    "UCT_C = 3                              # Amount of exploration. Apparently, for games with higher absolute rewards (e.g., Poker) this should be higher\n",
    "max_mcts_simulations = 100\n",
    "\n",
    "policy_epsilon = None #0.25            # What noise epsilon to use\n",
    "policy_alpha = None #1                 # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10\n",
    "omniscient_observer = True             # Whether the observation tensor input to the model is the total information (omniscient) or only the player's observation\n",
    "use_reward_policy = True               # Whether the MCTS policy should be weighted by reward or only the expore counts are taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert win_ratio_needed is None and average_reward_needed is not None or win_ratio_needed is not None and average_reward_needed is None, f\"win_ratio_needed and average_reward_needed are mutually exclusive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_config = MCTSConfig(\n",
    "    UCT_C, \n",
    "    max_mcts_simulations, \n",
    "    temperature, \n",
    "    temperature_drop, \n",
    "    policy_epsilon, \n",
    "    policy_alpha, \n",
    "    omniscient_observer=omniscient_observer, \n",
    "    use_reward_policy=use_reward_policy)\n",
    "\n",
    "evaluation_mcts_config = MCTSConfig(\n",
    "    UCT_C, \n",
    "    max_mcts_simulations, \n",
    "    0, \n",
    "    None, \n",
    "    None, \n",
    "    None, \n",
    "    omniscient_observer=omniscient_observer,\n",
    "    use_reward_policy=use_reward_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "model_type = 'mlp'\n",
    "nn_width = 64\n",
    "nn_depth = 2\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    game_name=game_name,\n",
    "    UCT_C=UCT_C,\n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    n_iterations=n_iterations,\n",
    "    \n",
    "    n_games_train=n_games_train,\n",
    "    n_games_valid=n_games_valid,\n",
    "    store_replays_every=store_replays_every,\n",
    "    \n",
    "    n_most_recent_train_samples=n_most_recent_train_samples,\n",
    "    n_most_recent_valid_samples=n_most_recent_valid_samples,\n",
    "    n_train_steps=n_train_steps,\n",
    "    n_valid_steps=n_valid_steps,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    n_evaluations=n_evaluations,\n",
    "    win_ratio_needed=win_ratio_needed,\n",
    "    average_reward_needed=average_reward_needed,\n",
    "    \n",
    "    policy_epsilon=policy_epsilon,\n",
    "    policy_alpha=policy_alpha,\n",
    "    \n",
    "    temperature=temperature,\n",
    "    temperature_drop=temperature_drop,\n",
    "    \n",
    "    model_type=model_type,\n",
    "    nn_width=nn_width,\n",
    "    nn_depth=nn_depth,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    \n",
    "    omniscient_observer=omniscient_observer,\n",
    "    use_reward_policy=use_reward_policy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray.shutdown()\n",
    "#ray.init(num_cpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and game\n",
    "run_name = generate_run_name(f'{LOGS_DIR}/{game_name}', game_prefix, match_arbitrary_suffixes=True)\n",
    "print(f\"Starting run: {run_name}\")\n",
    "\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "# Setup Model Manager\n",
    "model_config = OpenSpielModelConfig(\n",
    "    game, \n",
    "    model_type, \n",
    "    get_observation_tensor_shape(game, omniscient_observer), \n",
    "    nn_width, \n",
    "    nn_depth, \n",
    "    weight_decay, \n",
    "    learning_rate,\n",
    "    omniscient_observer=omniscient_observer)\n",
    "model_manager = OpenSpielCheckpointManager(game_name, run_name)\n",
    "model_manager.store_config(model_config)\n",
    "\n",
    "# Setup Evaluation Manager\n",
    "if ray.is_initialized():\n",
    "    evaluation_manager = ParallelEvaluationManager(game, model_manager, n_evaluations, evaluation_mcts_config)\n",
    "else:\n",
    "    evaluation_manager = EvaluationManager(game, n_evaluations, evaluation_mcts_config)\n",
    "    \n",
    "# Setup Replay Data Manager\n",
    "replay_data_manager = ReplayDataManager(model_manager.model_store_path)\n",
    "    \n",
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "rating_systems = [elo_rating_system, true_skill_rating_system]\n",
    "\n",
    "# Setup final training manager\n",
    "train_manager = AlphaZeroTrainManager(game, model_manager, evaluation_manager, n_most_recent_train_samples, n_most_recent_valid_samples, rating_systems)\n",
    "\n",
    "print(\"Num variables:\", train_manager.model_challenger.num_trainable_variables)\n",
    "train_manager.model_challenger.print_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}\")\n",
    "tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_samples, new_valid_samples = train_manager.generate_training_data(n_games_train, n_games_valid, mcts_config)\n",
    "    print(f'  - Generated {len(new_train_samples)} additional training samples and {len(new_valid_samples)} additional validation samples')\n",
    "    tensorboard.log_scalar(\"n_training_samples\", train_manager.replay_buffer.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    train_losses, valid_losses = train_manager.train_model(n_train_steps, n_valid_steps, batch_size, weight_decay)\n",
    "    print(f'  - Training: {mean_total_loss(train_losses[:int(len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(len(train_losses)/4):int(2 * len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(2 * len(train_losses)/4):int(3 * len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(3 * len(train_losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalars(\"Loss\", {\n",
    "        \"total/train\": mean([loss.total for loss in train_losses]),\n",
    "        \"policy/train\": mean([loss.policy for loss in train_losses]),\n",
    "        \"value/train\": mean([loss.value for loss in train_losses]),\n",
    "        \"total/valid\": mean([loss.total for loss in valid_losses]),\n",
    "        \"policy/valid\": mean([loss.policy for loss in valid_losses]),\n",
    "        \"value/valid\": mean([loss.value for loss in valid_losses])\n",
    "    }, iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes, challenger_average_reward = train_manager.evaluate_challenger_model()\n",
    "    \n",
    "    player_name_current_best = train_manager.get_player_name_current_best()\n",
    "    player_name_challenger = train_manager.get_player_name_challenger()\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    tensorboard.log_scalar(\"challenger_average_reward\", challenger_average_reward, iteration)\n",
    "    \n",
    "    # 4 Replace current best model with challenger model if it is better\n",
    "    train_manager.replace_model_with_challenger(challenger_win_rate, win_ratio_needed, challenger_average_reward, average_reward_needed)\n",
    "    if win_ratio_needed is not None:\n",
    "        if challenger_win_rate > win_ratio_needed:\n",
    "            print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "    elif average_reward_needed is not None:\n",
    "        if challenger_average_reward > average_reward_needed:\n",
    "            print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_average_reward:.2f} average reward)\")\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_train_samples])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalars(\"entropy\", {\n",
    "        \"current_best\": label_entropy,\n",
    "        \"challenger\": challenger_entropy}, iteration)\n",
    "    tensorboard.log_scalar(\"best_model_generation\", player_name_current_best, iteration)\n",
    "    \n",
    "    if iteration % store_replays_every == 0:\n",
    "        print(\"Replay buffer stored\")\n",
    "        replay_data_manager.store_replays(train_manager.replay_buffer, iteration)\n",
    "    tensorboard.flush()\n",
    "replay_data_manager.store_replays(train_manager.replay_buffer, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Blind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = replay_data_manager.load_replays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_model_config = OpenSpielModelConfig(game, 'mlp', replay_buffer.data[0].observation['player_observation'].shape, 128, 4, weight_decay=1e-5, learning_rate = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_model_manager = OpenSpielModelManager(game_name, f\"{run_name}-blind\").new_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_model_manager.store_config(blind_model_config)\n",
    "blind_model = blind_model_manager.build_model(blind_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_blind = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{blind_model_manager.get_run_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(100):\n",
    "    blind_losses = []\n",
    "    for _ in range(100):\n",
    "        sampled_train_inputs = replay_buffer.sample(batch_size*10, 'player_observation', n_most_recent=500)\n",
    "        loss = blind_model.update(sampled_train_inputs)\n",
    "        blind_losses.append(loss)\n",
    "    blind_model_manager.store_checkpoint(blind_model, iteration)\n",
    "    tensorboard_blind.log_scalars(\"Loss\", {\n",
    "        \"total/train\": mean([loss.total for loss in blind_losses]),\n",
    "        \"policy/train\": mean([loss.policy for loss in blind_losses]),\n",
    "        \"value/train\": mean([loss.value for loss in blind_losses])\n",
    "    }, iteration)\n",
    "    tensorboard_blind.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Investigation of specific game scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Kuhn Poker Comparison of Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(1)\n",
    "state.apply_action(0)\n",
    "state.apply_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_one.game.observer import OmniscientObserver\n",
    "from alpha_one.alg.imperfect_information import AlphaZeroOmniscientMCTSEvaluator\n",
    "from alpha_one.utils.mcts import compute_mcts_policy_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniscient_observer = OmniscientObserver(game)\n",
    "random_model = model_manager.build_model(model_config)\n",
    "mcts_bot = initialize_bot(game, train_manager.model_current_best, uct_c=UCT_C, max_simulations=max_mcts_simulations, omniscient_observer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Trained Omniscient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manager.model_current_best.inference([omniscient_observer.get_observation_tensor(state)], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Untrained Omniscient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model.inference([omniscient_observer.get_observation_tensor(state)], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. MCTS with trained Omniscient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = mcts_bot.mcts_search(state)\n",
    "print(root.total_reward / root.explore_count)\n",
    "policy = np.zeros(game.num_distinct_actions())\n",
    "for c in root.children:\n",
    "    if c.outcome is not None:\n",
    "        policy[c.action] = c.total_reward / c.explore_count\n",
    "    else:\n",
    "        policy[c.action] = c.total_reward / (c.explore_count - 1)  # If node is not a leaf, one explore count is used to unfold it. To get a proper average, we have to subtract that here\n",
    "\n",
    "policy = np.exp(policy) / np.sum(np.exp(policy))\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4. Trained Blind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_model.inference([state.observation_tensor(state.current_player())], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Leduc Poker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(0)\n",
    "state.apply_action(4)\n",
    "state.apply_action(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniscient_observer = OmniscientObserver(game)\n",
    "random_model = model_manager.build_model(model_config)\n",
    "mcts_bot = initialize_bot(game, train_manager.model_current_best, uct_c=UCT_C, max_simulations=max_mcts_simulations, omniscient_observer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Trained Omniscient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manager.model_current_best.inference([omniscient_observer.get_observation_tensor(state)], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2. Untrained Omniscient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model.inference([omniscient_observer.get_observation_tensor(state)], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3. MCTS with trained Omniscient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = mcts_bot.mcts_search(state)\n",
    "compute_mcts_policy(game, root, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4. Trained Blind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_model.inference([state.observation_tensor(state.current_player())], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Connect Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()\n",
    "state.apply_action(3)\n",
    "state.apply_action(3)\n",
    "state.apply_action(2)\n",
    "#state.apply_action(2)\n",
    "#state.apply_action(3)\n",
    "#state.apply_action(2)\n",
    "print(state.observation_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manager.model_challenger.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manager.model_current_best.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_inference(game, train_manager.model_challenger, state, uct_c=UCT_C, max_simulations=max_mcts_simulations, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = model_manager.load_model(427)\n",
    "model_loaded.inference([state.observation_tensor()], [state.legal_actions_mask()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_challenger_model(model_current_best, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alpha_one] *",
   "language": "python",
   "name": "conda-env-alpha_one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
