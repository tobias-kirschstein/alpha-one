{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "import math\n",
    "import ray\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.alpha_zero import model as model_lib\n",
    "from open_spiel.python.algorithms.alpha_zero import evaluator as evaluator_lib\n",
    "from open_spiel.python.algorithms import mcts\n",
    "\n",
    "from alpha_one.metrics import MatchOutcome, EloRatingSystem, TrueSkillRatingSystem, calculate_entropy\n",
    "from alpha_one.game.trajectory import GameTrajectory\n",
    "from alpha_one.game.buffer import ReplayBuffer\n",
    "from alpha_one.game.observer import get_observation_tensor_shape\n",
    "from alpha_one.utils.mcts import initialize_bot, compute_mcts_policy, play_one_game, mcts_inference\n",
    "from alpha_one.utils.logging import TensorboardLogger, generate_run_name\n",
    "from alpha_one.model.model_manager import OpenSpielCheckpointManager, OpenSpielModelManager\n",
    "from alpha_one.model.evaluation import EvaluationManager, ParallelEvaluationManager\n",
    "from alpha_one.model.config import OpenSpielModelConfig\n",
    "from alpha_one.train import AlphaZeroTrainManager, MCTSConfig\n",
    "from alpha_one.data.replay import ReplayDataManager\n",
    "from env import MODEL_SAVES_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is done in a similar way as AlphaZero.\n",
    "\n",
    "Some Remarks:\n",
    "1. determinized_MCTS and omniscient_observer in MCTSConfig should be set to \"True\" while training\n",
    "2. See AlphaZeroTrainManager and utilis/determinized_mcts for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'leduc_poker'\n",
    "game_prefix = 'LP-local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50                     # How often the whole procedure is repeated. Also corresponds to the number of evaluations\n",
    "\n",
    "# Train samples generation\n",
    "n_games_train = 100             # How many new states will be generated by the best model via self-play for training (Training set size delta). Has to be larger than batch_size\n",
    "n_games_valid = 10\n",
    "store_replays_every = 10\n",
    "\n",
    "# Model update\n",
    "n_most_recent_train_samples = 50000    # Among which training samples to choose to train current model\n",
    "n_most_recent_valid_samples = 50000\n",
    "n_train_steps = 40                     # After how many gradient updates the new model tries to beat the current best\n",
    "n_valid_steps = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Evaluation\n",
    "n_evaluations = 10                    # How many games should be played to measure which model is better\n",
    "evaluation_strategy = 'mcts'           # 'best_response'\n",
    "win_ratio_needed = 0.55                # Minimum win ratio that the challenger model needs in order to supersede the current best model\n",
    "\n",
    "# MCTS config\n",
    "UCT_C = math.sqrt(2)\n",
    "max_mcts_simulations = 100\n",
    "\n",
    "policy_epsilon = None #0.25            # What noise epsilon to use\n",
    "policy_alpha = None #1                 # What dirichlet noise alpha to use\n",
    "\n",
    "temperature = 1\n",
    "temperature_drop = 10\n",
    "omniscient_observer = True\n",
    "use_reward_policy = True\n",
    "determinized_MCTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_config = MCTSConfig(UCT_C, max_mcts_simulations, temperature, temperature_drop, policy_epsilon, policy_alpha, omniscient_observer=omniscient_observer, use_reward_policy=use_reward_policy)\n",
    "evaluation_mcts_config = MCTSConfig(UCT_C, max_mcts_simulations, 0, None, None, None, determinized_MCTS=determinized_MCTS, omniscient_observer=omniscient_observer, use_reward_policy=use_reward_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "model_type = 'mlp'\n",
    "nn_width = 64\n",
    "nn_depth = 2\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    game_name=game_name,\n",
    "    UCT_C=UCT_C,\n",
    "    max_mcts_simulations=max_mcts_simulations,\n",
    "    n_iterations=n_iterations,\n",
    "    \n",
    "    n_games_train=n_games_train,\n",
    "    n_games_valid=n_games_valid,\n",
    "    store_replays_every=store_replays_every,\n",
    "    \n",
    "    n_most_recent_train_samples=n_most_recent_train_samples,\n",
    "    n_most_recent_valid_samples=n_most_recent_valid_samples,\n",
    "    n_train_steps=n_train_steps,\n",
    "    n_valid_steps=n_valid_steps,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    n_evaluations=n_evaluations,\n",
    "    win_ratio_needed=win_ratio_needed,\n",
    "    \n",
    "    policy_epsilon=policy_epsilon,\n",
    "    policy_alpha=policy_alpha,\n",
    "    \n",
    "    temperature=temperature,\n",
    "    temperature_drop=temperature_drop,\n",
    "    \n",
    "    model_type=model_type,\n",
    "    nn_width=nn_width,\n",
    "    nn_depth=nn_depth,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    \n",
    "    omniscient_observer=omniscient_observer,\n",
    "    determinized_MCTS=determinized_MCTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_loss(losses):\n",
    "    return mean([loss.total for loss in losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and game\n",
    "run_name = generate_run_name(f'{LOGS_DIR}/{game_name}', game_prefix)\n",
    "print(f\"Starting run: {run_name}\")\n",
    "\n",
    "game = pyspiel.load_game(game_name)\n",
    "\n",
    "# Setup Model Manager\n",
    "model_config = OpenSpielModelConfig(\n",
    "    game, \n",
    "    model_type, \n",
    "    get_observation_tensor_shape(game, omniscient_observer), \n",
    "    nn_width, \n",
    "    nn_depth, \n",
    "    weight_decay, \n",
    "    learning_rate,\n",
    "    omniscient_observer=omniscient_observer)\n",
    "model_manager = OpenSpielCheckpointManager(game_name, run_name)\n",
    "model_manager.store_config(model_config)\n",
    "\n",
    "# Setup Evaluation Manager\n",
    "if ray.is_initialized():\n",
    "    evaluation_manager = ParallelEvaluationManager(game, model_manager, n_evaluations, evaluation_mcts_config)\n",
    "else:\n",
    "    evaluation_manager = EvaluationManager(game, n_evaluations, evaluation_mcts_config)\n",
    "    \n",
    "    \n",
    "# Setup rating systems for evaluation\n",
    "elo_rating_system = EloRatingSystem(40)\n",
    "true_skill_rating_system = TrueSkillRatingSystem()\n",
    "rating_systems = [elo_rating_system, true_skill_rating_system]\n",
    "\n",
    "# Setup final training manager\n",
    "train_manager = AlphaZeroTrainManager(game, model_manager, evaluation_manager, n_most_recent_train_samples, n_most_recent_valid_samples, rating_systems)\n",
    "\n",
    "print(\"Num variables:\", train_manager.model_challenger.num_trainable_variables)\n",
    "train_manager.model_challenger.print_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorboardLogger(f\"{LOGS_DIR}/{game_name}/{run_name}\")\n",
    "tensorboard.log_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # 1 Generate training data with current best model\n",
    "    new_train_samples, new_valid_samples = train_manager.generate_training_data(n_games_train, n_games_valid, mcts_config)\n",
    "    print(f'  - Generated {len(new_train_samples)} additional training samples and {len(new_valid_samples)} additional validation samples')\n",
    "    tensorboard.log_scalar(\"n_training_samples\", train_manager.replay_buffer.get_total_samples(), iteration)\n",
    "    \n",
    "    # 2 Repeatedly sample from training set and update weights on current model\n",
    "    train_losses, valid_losses = train_manager.train_model(n_train_steps, n_valid_steps, batch_size, weight_decay)\n",
    "    print(f'  - Training: {mean_total_loss(train_losses[:int(len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(len(train_losses)/4):int(2 * len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(2 * len(train_losses)/4):int(3 * len(train_losses)/4)]):.2f} \\\n",
    "            -> {mean_total_loss(train_losses[int(3 * len(train_losses)/4):]):.2f}')\n",
    "    tensorboard.log_scalars(\"Loss\", {\n",
    "        \"total/train\": mean([loss.total for loss in train_losses]),\n",
    "        \"policy/train\": mean([loss.policy for loss in train_losses]),\n",
    "        \"value/train\": mean([loss.value for loss in train_losses]),\n",
    "        \"total/valid\": mean([loss.total for loss in valid_losses]),\n",
    "        \"policy/valid\": mean([loss.policy for loss in valid_losses]),\n",
    "        \"value/valid\": mean([loss.value for loss in valid_losses])\n",
    "    }, iteration)\n",
    "    \n",
    "    # 3 Evaluate trained model against current best model\n",
    "    challenger_win_rate, challenger_policies, match_outcomes = train_manager.evaluate_challenger_model()\n",
    "    \n",
    "    player_name_current_best = train_manager.get_player_name_current_best()\n",
    "    player_name_challenger = train_manager.get_player_name_challenger()\n",
    "    \n",
    "    true_skill_rating_system.update_ratings(match_outcomes)\n",
    "    elo_rating_system.update_ratings(match_outcomes)\n",
    "    print(f\"  - Ratings current best: {true_skill_rating_system.get_rating(player_name_current_best)}, {elo_rating_system.get_rating(player_name_current_best):0.3f}\")\n",
    "    print(f\"  - Ratings challenger: {true_skill_rating_system.get_rating(player_name_challenger)}, {elo_rating_system.get_rating(player_name_challenger):0.3f}\")\n",
    "    tensorboard.log_scalars(\"elo_rating\", {\n",
    "        \"current_best\": elo_rating_system.get_rating(player_name_current_best),\n",
    "        \"challenger\": elo_rating_system.get_rating(player_name_challenger)\n",
    "    }, iteration)\n",
    "    tensorboard.log_scalars(\"true_skill_rating\", {\n",
    "        \"current_best\": true_skill_rating_system.get_rating(player_name_current_best).mu,\n",
    "        \"challenger\": true_skill_rating_system.get_rating(player_name_challenger).mu\n",
    "    }, iteration)\n",
    "    \n",
    "    print(f'  - Challenger won {int(round(challenger_win_rate * n_evaluations))}/{n_evaluations} games ({challenger_win_rate:.2%} win rate)')\n",
    "    tensorboard.log_scalar(\"challenger_win_rate\", challenger_win_rate, iteration)\n",
    "    \n",
    "    # 4 Replace current best model with challenger model if it is better\n",
    "    train_manager.replace_model_with_challenger(challenger_win_rate, win_ratio_needed)\n",
    "    if challenger_win_rate > win_ratio_needed:\n",
    "        print(f\"  - Model at iteration {iteration} supersedes previous model ({challenger_win_rate:.2%} win rate)\")\n",
    "        \n",
    "    challenger_entropy = calculate_entropy(challenger_policies)\n",
    "    print(f\"  - Challenger entropy: {challenger_entropy:0.3f}\")\n",
    "    label_entropy = calculate_entropy([sample.policy for sample in new_train_samples])\n",
    "    print(f\"  - Label entropy: {label_entropy:0.3f}\")\n",
    "    \n",
    "    tensorboard.log_scalars(\"entropy\", {\n",
    "        \"current_best\": label_entropy,\n",
    "        \"challenger\": challenger_entropy}, iteration)\n",
    "    tensorboard.log_scalar(\"best_model_generation\", player_name_current_best, iteration)\n",
    "    \n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
