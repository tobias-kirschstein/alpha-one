# tum-adlr-ws20-9
This is the repository for the project `AlphaOne - Self-play Reinforcement Learning on Imperfect Information Games` conducted at TUM in winter 2020.

# 1 Contributing
The repository structure is as follows:
 * `notebooks/`: All Jupyter notebooks go here. Please number them in ascending order, i.e., `xx_notebook_name.ipynb`, with leading zeros. Also, introduce subfolders for a logical clustering of the notebooks
 * `alpha_one/`: All the python modules and classes go here
 * `scripts/`: Any shell or Python scripts that are supposed to be run from the commandline go here
 * `tests/`: Any tests to ensure or debug program functionality go here  
 * `setup.py`: Add any newly introduced dependency to a library that has to be installed via `pip` here
 * `env.yaml` and `env.py`: Add any environment variables (such as paths to dataset files etc.) to the `env.yaml` file. To allow quick usage, load them in `env.py` and import them in your script via `from env import VARIABLE_NAME`
 * `.gitignore`: Add any files and folders that should not be versioned to the `.gitignore`. Typically, datasets, binary files and the like should be ignored by git

# 2 Setting Up
* Clone the tum-adlr-ws20-9 repository to your local machine:\
`git clone https://gitlab.lrz.de/ge49muj/tum-adlr-ws20-9`
* Clone the OpenSpiel repository to your local machine:\
`git clone https://github.com/deepmind/open_spiel`
* Create a virtual environment running python version 3.8.5:\
`conda create -n adlr python=3.8.5`\
`conda activate adlr`
* Install required packages in your virtual environment:\
`pip install -r requirements.txt`
* Set your PYTHONPATH environment variable:\
`export PYTHONPATH=$PYTHONPATH:/<path_to_open_spiel>`\
`export PYTHONPATH=$PYTHONPATH:/<path_to_open_spiel>/build/python`

# 3 Training and Evaluation of the models

* Set "model_saves_dir", "logs_dir" and "plots_dir" in `env.yaml`.
* Run scripts inside `scripts/`.
* For evaluations, use `notebooks/evaluation` 

# 4 Additional resources
## 4.1 Trouble Shooting

If you get:
`pyglet.canvas.xlib.NoSuchDisplayException: Cannot connect to "None"`

because you use Gym inside Ubuntu WSL, then install Xming https://sourceforge.net/projects/xming/ on the Windows host and add "export DISPLAY=localhost:0.0" to your .bashrc file in Ubuntu


## 4.2 Downloading ROMs for retro

Download zip file from http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html
Open Roms.rar > ROMS.rar and find Space Invaders (1980) XXXXXX
Extract all matches (there are 5 of them) into your destin folder
python -m retro.import . (don't forget the point)

## 4.3 AlphaZero Implementation

https://github.com/petosa/multiplayer-alphazero

## 4.4 Tips on RL (taken from anywhere)
 * most of the time for most gym environments three Linear layers is enough, maybe 64 ~ 500 neurons per level and I would suggest you use a pyramid like structure. Conv2d is only necessary for visual inputs.

 * Must use discount < 1, otherwise there is no garantee on convergence, because the convergence of magic like RL algorithms relies on a simple math principle, you must have learned it in your freshman math analysis class: converging series or alittle bit more advanced compaction

 * Because the naive REINFORCE algorithm is bad, try use DQN, RAINBOW, DDPG,TD3, A2C, A3C, PPO, TRPO, ACKTR or whatever you like. Follow the train result reference openai gym train reference, normally you need to let the agent interact with the environment for 100K or even 1M steps, for extremely complex real life scenes, you need stacks of servers and massively distributed algorithms like IMPALA. There are many many many many methods to learn faster, but I would recommend you to start with PPO. But PPO is not a solution once and for all, it cannot solve some scenes like the hardcore bipedalwalker from openai gym.

 * You will know that it is learning, by looking at its behavior changing from a total noise, to a fool, to an expert, and a fool again. Reward and loss might be good indicators, but be careful of your reward design, as your networks will exploit it and take lazy counter measures! Thatsâ€™ called reward shaping.

 * Please use batches, because it can stablelize training, because pytorch will average the gradients computed from each sample from the batch, You see more, you can judge better, right? normally 100~256 samples per batch, but some studies says monsterous batchsize could be helpful like 20000+
